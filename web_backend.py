"""
Web Backend for Watermark Remover
Modular backend that processes images with different mask generation methods
"""
import os
import sys
import yaml
import torch
import cv2
import numpy as np
from pathlib import Path
from PIL import Image, ImageDraw
from typing import Dict, Any, Optional, Tuple, Union
import io
import logging
from dataclasses import dataclass

# Import existing modules
from transformers import AutoProcessor, AutoModelForCausalLM
from iopaint.model_manager import ModelManager
from iopaint.schema import HDStrategy, LDMSampler, InpaintRequest as Config

# Import mask generators
sys.path.append(str(Path(__file__).parent.parent / "Watermark_Remover_KK"))
sys.path.append(str(Path(__file__).parent.parent / "Watermark_sam" / "watermark-segmentation"))

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s | %(levelname)-8s | %(name)s:%(funcName)s:%(lineno)d - %(message)s',
    datefmt='%Y-%m-%d %H:%M:%S'
)
logger = logging.getLogger(__name__)

@dataclass
class ProcessingResult:
    """ç»“æœæ•°æ®ç±»"""
    success: bool
    result_image: Optional[Image.Image] = None
    mask_image: Optional[Image.Image] = None
    error_message: Optional[str] = None
    processing_time: Optional[float] = None

class CustomMaskGenerator:
    """è‡ªå®šä¹‰maskç”Ÿæˆå™¨ï¼ŒåŸºäºWatermark_samæ¨¡å‹"""
    
    def __init__(self, config: Dict[str, Any]):
        self.config = config
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        self._load_model()
        self._setup_preprocessing()
    
    def _load_model(self):
        """åŠ è½½è‡ªå®šä¹‰åˆ†å‰²æ¨¡å‹"""
        try:
            import segmentation_models_pytorch as smp
            import albumentations as A
            from albumentations.pytorch import ToTensorV2
            
            # å®šä¹‰æ¨¡å‹æ¶æ„ï¼ˆä¸Produce_mask.pyä¸€è‡´ï¼‰
            class WMModel(torch.nn.Module):
                def __init__(self, freeze_encoder=True):
                    super().__init__()
                    self.net = smp.create_model("FPN", encoder_name="mit_b5", in_channels=3, classes=1)
                    if freeze_encoder:
                        for p in self.net.encoder.parameters():
                            p.requires_grad = False

                def forward(self, x):
                    return self.net(x)
            
            # åŠ è½½æ¨¡å‹
            self.model = WMModel(freeze_encoder=False).to(self.device)
            
            # åŠ è½½checkpoint
            mask_config = self.config['mask_generator']
            ckpt_path = mask_config['mask_model_path']
            
            if not os.path.exists(ckpt_path):
                raise FileNotFoundError(f"Model checkpoint not found: {ckpt_path}")
                
            ckpt = torch.load(ckpt_path, map_location=self.device)
            state_dict = {k.replace("net.", ""): v for k, v in ckpt["state_dict"].items() if k.startswith("net.")}
            self.model.net.load_state_dict(state_dict)
            self.model.eval()
            
            logger.info(f"Custom mask model loaded from: {ckpt_path}")
            
        except Exception as e:
            logger.error(f"Failed to load custom mask model: {e}")
            raise
    
    def _setup_preprocessing(self):
        """è®¾ç½®é¢„å¤„ç†ç®¡é“"""
        import albumentations as A
        from albumentations.pytorch import ToTensorV2
        
        mask_config = self.config['mask_generator']
        self.image_size = mask_config['image_size']
        self.imagenet_mean = mask_config['imagenet_mean']
        self.imagenet_std = mask_config['imagenet_std']
        self.mask_threshold = mask_config['mask_threshold']
        
        self.aug_val = A.Compose([
            A.LongestMaxSize(max_size=self.image_size),
            A.PadIfNeeded(min_height=self.image_size, min_width=self.image_size, border_mode=cv2.BORDER_CONSTANT),
            A.Normalize(mean=self.imagenet_mean, std=self.imagenet_std, max_pixel_value=255.0),
            ToTensorV2(),
        ])
    
    def generate_mask(self, image: Image.Image) -> Image.Image:
        """ç”Ÿæˆæ°´å°mask"""
        try:
            # è½¬æ¢ä¸ºnumpyæ•°ç»„
            image_rgb = np.array(image.convert("RGB"))
            orig_h, orig_w = image_rgb.shape[:2]
            
            # é¢„å¤„ç†
            sample = self.aug_val(image=image_rgb, mask=None)
            img_tensor = sample["image"].unsqueeze(0).to(self.device)
            
            # æ¨¡å‹æ¨ç†
            with torch.no_grad():
                pred_mask = self.model(img_tensor)
                pred_mask = torch.sigmoid(pred_mask).squeeze().cpu().numpy()
            
            # åå¤„ç†ï¼šæ­£ç¡®è®¡ç®—ç¼©æ”¾å’Œpaddingä¿¡æ¯
            scale = min(self.image_size / orig_h, self.image_size / orig_w)
            new_h, new_w = int(orig_h * scale), int(orig_w * scale)
            
            # è®¡ç®—å®é™…ä½¿ç”¨çš„paddingï¼ˆä¸aug_valå’ŒProduce_mask.pyä¿æŒä¸€è‡´ï¼‰
            pad_top = (self.image_size - new_h) // 2
            pad_left = (self.image_size - new_w) // 2
            
            # è£å‰ªæ‰paddingï¼Œæ¢å¤åˆ°ç¼©æ”¾åçš„å°ºå¯¸ï¼ˆä¸åŸç‰ˆå®ç°å®Œå…¨ä¸€è‡´ï¼‰
            pred_crop = pred_mask[pad_top:pad_top+new_h, pad_left:pad_left+new_w]
            
            # ç›´æ¥ç¼©æ”¾å›åŸå°ºå¯¸ï¼Œé¿å…å½¢å˜
            pred_mask = cv2.resize(pred_crop, (orig_w, orig_h), interpolation=cv2.INTER_LINEAR)
            
            # äºŒå€¼åŒ–
            binary_mask = (pred_mask > self.mask_threshold).astype(np.uint8) * 255
            
            # è†¨èƒ€å¤„ç†
            dilate_size = self.config['mask_generator'].get('mask_dilate_kernel_size', 3)
            if dilate_size > 0:
                kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (dilate_size, dilate_size))
                binary_mask = cv2.dilate(binary_mask, kernel, iterations=1)
            
            # è½¬æ¢ä¸ºPIL Image
            return Image.fromarray(binary_mask, mode='L')
            
        except Exception as e:
            logger.error(f"Custom mask generation failed: {e}")
            raise

class FlorenceMaskGenerator:
    """Florence-2 maskç”Ÿæˆå™¨ï¼ˆåŸç‰ˆï¼‰"""
    
    def __init__(self, config: Dict[str, Any]):
        self.config = config
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        self._load_model()
    
    def _load_model(self):
        """åŠ è½½Florence-2æ¨¡å‹"""
        try:
            model_name = self.config['models']['florence_model']
            self.florence_model = AutoModelForCausalLM.from_pretrained(
                model_name, trust_remote_code=True
            ).to(self.device).eval()
            self.florence_processor = AutoProcessor.from_pretrained(
                model_name, trust_remote_code=True
            )
            logger.info(f"Florence-2 model loaded: {model_name}")
        except Exception as e:
            logger.error(f"Failed to load Florence-2 model: {e}")
            raise
    
    def generate_mask(self, image: Image.Image, max_bbox_percent: float = 10.0, detection_prompt: str = "watermark") -> Image.Image:
        """ä½¿ç”¨Florence-2ç”Ÿæˆmask"""
        try:
            from utils import TaskType, identify
            
            # æ£€æµ‹æ°´å°ï¼ˆä½¿ç”¨è‡ªå®šä¹‰promptï¼‰
            text_input = detection_prompt
            task_prompt = TaskType.OPEN_VOCAB_DETECTION
            logger.info(f"ğŸ¤– Florence-2æ£€æµ‹: ä½¿ç”¨prompt '{text_input}', max_bbox_percent={max_bbox_percent}")
            parsed_answer = identify(task_prompt, image, text_input, 
                                   self.florence_model, self.florence_processor, 
                                   str(self.device))
            
            # åˆ›å»ºmask
            mask = Image.new("L", image.size, 0)
            draw = ImageDraw.Draw(mask)
            
            detection_key = "<OPEN_VOCABULARY_DETECTION>"
            if detection_key in parsed_answer and "bboxes" in parsed_answer[detection_key]:
                image_area = image.width * image.height
                for bbox in parsed_answer[detection_key]["bboxes"]:
                    x1, y1, x2, y2 = map(int, bbox)
                    bbox_area = (x2 - x1) * (y2 - y1)
                    if (bbox_area / image_area) * 100 <= max_bbox_percent:
                        draw.rectangle([x1, y1, x2, y2], fill=255)
                    else:
                        logger.warning(f"Skipping large bounding box: {bbox}")
            
            return mask
            
        except Exception as e:
            logger.error(f"Florence mask generation failed: {e}")
            raise

class WatermarkProcessor:
    """æ°´å°å¤„ç†ä¸»ç±»"""
    
    def __init__(self, config_path: str = "web_config.yaml"):
        self.config = self._load_config(config_path)
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        
        # åˆå§‹åŒ–maskç”Ÿæˆå™¨
        mask_type = self.config['mask_generator']['model_type']
        if mask_type == "custom":
            self.mask_generator = CustomMaskGenerator(self.config)
        else:
            self.mask_generator = FlorenceMaskGenerator(self.config)
        
        # åˆå§‹åŒ–inpaintingæ¨¡å‹
        self.model_manager = None
        self._load_lama_model()
    
    def _load_config(self, config_path: str) -> Dict[str, Any]:
        """åŠ è½½é…ç½®æ–‡ä»¶"""
        try:
            with open(config_path, 'r', encoding='utf-8') as f:
                return yaml.safe_load(f)
        except Exception as e:
            logger.error(f"Failed to load config: {e}")
            raise
    
    def _load_lama_model(self):
        """åŠ è½½LaMA inpaintingæ¨¡å‹"""
        try:
            model_name = self.config['models']['lama_model']
            self.model_manager = ModelManager(name=model_name, device=str(self.device))
            logger.info(f"LaMA model loaded: {model_name}")
        except Exception as e:
            logger.error(f"Failed to load LaMA model: {e}")
            raise
    
    @staticmethod
    def load_image_opencv(image_source: Union[str, io.BytesIO, bytes]) -> np.ndarray:
        """ä½¿ç”¨OpenCVç›´æ¥è¯»å–å›¾åƒï¼Œé¿å…PILçš„è‰²å½©æ ¡æ­£"""
        logger.info(f"ğŸ” load_image_opencv: è¾“å…¥ç±»å‹ {type(image_source)}")
        
        if isinstance(image_source, str):
            # æ–‡ä»¶è·¯å¾„
            logger.info(f"ğŸ“ ä»æ–‡ä»¶è·¯å¾„è¯»å–: {image_source}")
            image = cv2.imread(image_source)
            logger.info(f"   cv2.imreadç»“æœ: shape={image.shape if image is not None else 'None'}, dtype={image.dtype if image is not None else 'None'}")
            return image
        elif isinstance(image_source, (io.BytesIO, bytes)):
            # BytesIOæˆ–å­—èŠ‚æ•°æ®
            logger.info(f"ğŸ’¾ ä»BytesIO/bytesè¯»å–")
            if isinstance(image_source, io.BytesIO):
                bytes_data = image_source.getvalue()
                logger.info(f"   BytesIO.getvalue() æ•°æ®é•¿åº¦: {len(bytes_data)}")
            else:
                bytes_data = image_source
                logger.info(f"   ç›´æ¥bytesæ•°æ®é•¿åº¦: {len(bytes_data)}")
            
            bytes_array = np.asarray(bytearray(bytes_data), dtype=np.uint8)
            logger.info(f"   è½¬æ¢ä¸ºnumpyæ•°ç»„: shape={bytes_array.shape}, dtype={bytes_array.dtype}")
            
            image = cv2.imdecode(bytes_array, cv2.IMREAD_COLOR)
            logger.info(f"   cv2.imdecodeç»“æœ: shape={image.shape if image is not None else 'None'}, dtype={image.dtype if image is not None else 'None'}")
            if image is not None:
                logger.info(f"   è§£ç åç¬¬ä¸€ä¸ªåƒç´  (BGR): {image[0,0]}")
            return image
        else:
            raise ValueError(f"Unsupported image source type: {type(image_source)}")

    def process_image(self, 
                     image: Union[Image.Image, str, io.BytesIO, bytes],
                     transparent: bool = False,
                     max_bbox_percent: float = 10.0,
                     detection_prompt: str = "watermark",
                     force_format: Optional[str] = None,
                     custom_inpaint_config: Optional[Dict[str, Any]] = None) -> ProcessingResult:
        """å¤„ç†å•å¼ å›¾ç‰‡"""
        import time
        start_time = time.time()
        
        logger.info("ğŸš€ å¼€å§‹å¤„ç†å›¾åƒ...")
        logger.info(f"ğŸ“¸ è¾“å…¥å›¾åƒç±»å‹: {type(image)}")
        logger.info(f"ğŸ¯ å¤„ç†æ¨¡å¼: {'é€æ˜' if transparent else 'ä¿®å¤'}")
        logger.info(f"ğŸ“‹ è‡ªå®šä¹‰é…ç½®: {custom_inpaint_config}")
        
        try:
            # ä¿®æ­£å›¾åƒé¢„å¤„ç† - éµå¾ªiopaintè¦æ±‚ï¼ˆRGBè¾“å…¥ï¼‰
            logger.info("ğŸ”„ å¼€å§‹ä¿®æ­£å›¾åƒé¢„å¤„ç†...")
            
            if isinstance(image, (str, io.BytesIO, bytes)):
                logger.info("ğŸ“ æ£€æµ‹åˆ°æ–‡ä»¶è·¯å¾„/BytesIO/bytesè¾“å…¥ï¼Œä½¿ç”¨OpenCVè¯»å–ç„¶åè½¬RGB")
                # ä½¿ç”¨OpenCVè¯»å–BGRæ ¼å¼ï¼Œä½†è¦è½¬æ¢ä¸ºRGBç»™iopaint
                image_bgr = self.load_image_opencv(image)
                logger.info(f"   OpenCVè¯»å–BGR: shape={image_bgr.shape}, ç¬¬ä¸€ä¸ªåƒç´ ={image_bgr[0,0]}")
                
                # è½¬æ¢ä¸ºRGBç”¨äºLaMAå¤„ç†ï¼ˆiopaintè¦æ±‚ï¼‰
                image_rgb = cv2.cvtColor(image_bgr, cv2.COLOR_BGR2RGB)
                logger.info(f"   BGRâ†’RGBè½¬æ¢: shape={image_rgb.shape}, ç¬¬ä¸€ä¸ªåƒç´ ={image_rgb[0,0]}")
                
                # ä¸ºmaskç”Ÿæˆåˆ›å»ºPILç‰ˆæœ¬
                image_pil = Image.fromarray(image_rgb)
                logger.info(f"   PILå›¾åƒï¼ˆç”¨äºmaskç”Ÿæˆï¼‰: size={image_pil.size}")
                
                # ä¸»è¦å¤„ç†æµç¨‹ä½¿ç”¨RGBæ•°æ®ï¼ˆiopaintè¦æ±‚ï¼‰
                image_for_processing = image_rgb
            else:
                logger.info("ğŸ–¼ï¸ æ£€æµ‹åˆ°PILå›¾åƒè¾“å…¥")
                # PILå›¾åƒè¾“å…¥ - ç›´æ¥ä½¿ç”¨RGBæ ¼å¼ï¼ˆiopaintè¦æ±‚ï¼‰
                image_pil = image
                image_rgb = np.array(image_pil.convert("RGB"))
                logger.info(f"   PILâ†’RGBæ•°ç»„: shape={image_rgb.shape}, ç¬¬ä¸€ä¸ªåƒç´ ={image_rgb[0,0]}")
                
                # ä¸»è¦å¤„ç†æµç¨‹ä½¿ç”¨RGBæ•°æ®ï¼ˆiopaintè¦æ±‚ï¼‰
                image_for_processing = image_rgb
            
            # ç”Ÿæˆmask
            logger.info("ğŸ­ å¼€å§‹ç”Ÿæˆmask...")
            if isinstance(self.mask_generator, CustomMaskGenerator):
                logger.info("ğŸ”§ ä½¿ç”¨Custom Mask Generator")
                mask_image = self.mask_generator.generate_mask(image_pil)
            else:
                logger.info("ğŸ¤– ä½¿ç”¨Florence-2 Mask Generator")
                mask_image = self.mask_generator.generate_mask(image_pil, max_bbox_percent, detection_prompt)
            
            logger.info(f"âœ… Maskç”Ÿæˆå®Œæˆ, size: {mask_image.size}, mode: {mask_image.mode}")
            
            if transparent:
                logger.info("ğŸ«¥ å¼€å§‹é€æ˜å¤„ç†...")
                # é€æ˜å¤„ç†
                result_image = self._make_region_transparent(image_pil, mask_image)
                logger.info(f"âœ… é€æ˜å¤„ç†å®Œæˆ, size: {result_image.size}, mode: {result_image.mode}")
            else:
                logger.info("ğŸ¤– å¼€å§‹LaMAä¿®å¤å¤„ç†...")
                # LaMA inpaintingï¼ˆä½¿ç”¨BGRæ ¼å¼ - æœ€ä½³å®è·µï¼‰
                logger.info("ğŸ”„ ä½¿ç”¨BGRæ ¼å¼æ•°æ®è¿›è¡ŒLaMAå¤„ç†ï¼ˆæœ€ä½³å®è·µï¼‰")
                result_image = self._process_with_lama(image_for_processing, mask_image, custom_inpaint_config)
                
                logger.info(f"âœ… LaMAä¿®å¤å®Œæˆ, size: {result_image.size}, mode: {result_image.mode}")
            
            processing_time = time.time() - start_time
            logger.info(f"â±ï¸ æ€»å¤„ç†æ—¶é—´: {processing_time:.2f}ç§’")
            
            return ProcessingResult(
                success=True,
                result_image=result_image,
                mask_image=mask_image,
                processing_time=processing_time
            )
            
        except Exception as e:
            processing_time = time.time() - start_time
            logger.error(f"âŒ å›¾åƒå¤„ç†å¤±è´¥: {e}")
            logger.error(f"â±ï¸ å¤„ç†æ—¶é—´: {processing_time:.2f}ç§’")
            return ProcessingResult(
                success=False,
                error_message=str(e),
                processing_time=processing_time
            )
    
    def _make_region_transparent(self, image: Image.Image, mask: Image.Image) -> Image.Image:
        """ä½¿åŒºåŸŸé€æ˜ï¼ˆä¸remwm.pyä¸­çš„å‡½æ•°ä¸€è‡´ï¼‰"""
        image = image.convert("RGBA")
        mask = mask.convert("L")
        transparent_image = Image.new("RGBA", image.size)
        
        for x in range(image.width):
            for y in range(image.height):
                if mask.getpixel((x, y)) > 0:
                    transparent_image.putpixel((x, y), (0, 0, 0, 0))
                else:
                    transparent_image.putpixel((x, y), image.getpixel((x, y)))
        
        return transparent_image
    
    def _process_with_lama(self, image: Union[Image.Image, np.ndarray, str], mask: Union[Image.Image, np.ndarray], custom_config: Optional[Dict[str, Any]] = None) -> Image.Image:
        """ä½¿ç”¨LaMAè¿›è¡Œinpaintingï¼ˆæ”¯æŒè‡ªå®šä¹‰é…ç½®ï¼‰- é‡‡ç”¨test_direct_inpaintæœ€ä½³å®è·µ"""
        logger.info("ğŸ” å¼€å§‹LaMAå¤„ç†æµç¨‹...")
        
        # é»˜è®¤é…ç½®ï¼ˆä¸test_direct_inpaint.pyä¿æŒä¸€è‡´ï¼‰
        default_config = {
            'ldm_steps': 50,
            'ldm_sampler': 'ddim',
            'hd_strategy': 'CROP',
            'hd_strategy_crop_margin': 64,
            'hd_strategy_crop_trigger_size': 800,
            'hd_strategy_resize_limit': 1600,
        }
        
        # åˆå¹¶è‡ªå®šä¹‰é…ç½®
        if custom_config:
            default_config.update(custom_config)
            logger.info(f"ğŸ“‹ ä½¿ç”¨è‡ªå®šä¹‰é…ç½®: {custom_config}")
        
        # è½¬æ¢ç­–ç•¥æšä¸¾
        strategy_map = {
            'CROP': HDStrategy.CROP,
            'RESIZE': HDStrategy.RESIZE,
            'ORIGINAL': HDStrategy.ORIGINAL
        }
        
        sampler_map = {
            'ddim': LDMSampler.ddim,
            'plms': LDMSampler.plms
        }
        
        config = Config(
            ldm_steps=default_config['ldm_steps'],
            ldm_sampler=sampler_map.get(default_config['ldm_sampler'], LDMSampler.ddim),
            hd_strategy=strategy_map.get(default_config['hd_strategy'], HDStrategy.CROP),
            hd_strategy_crop_margin=default_config['hd_strategy_crop_margin'],
            hd_strategy_crop_trigger_size=default_config['hd_strategy_crop_trigger_size'],
            hd_strategy_resize_limit=default_config['hd_strategy_resize_limit'],
        )
        
        logger.info(f"âš™ï¸ LaMAé…ç½®: ldm_steps={config.ldm_steps}, sampler={config.ldm_sampler}, strategy={config.hd_strategy}")
        
        # ç®€åŒ–çš„é¢œè‰²å¤„ç†æµç¨‹ - éµå¾ªiopaintè¦æ±‚
        logger.info("ğŸ¨ å¼€å§‹LaMAå¤„ç†æµç¨‹...")
        
        # â‘  å¤„ç†å›¾åƒè¾“å…¥ - ç¡®ä¿ä¼ å…¥RGBæ ¼å¼ç»™iopaint
        logger.info(f"ğŸ“¸ è¾“å…¥å›¾åƒç±»å‹: {type(image)}")
        
        if isinstance(image, np.ndarray):
            # numpyæ•°ç»„è¾“å…¥ï¼Œåº”è¯¥å·²ç»æ˜¯RGBæ ¼å¼ï¼ˆä»process_imageä¼ å…¥ï¼‰
            logger.info(f"ğŸ”¢ RGBæ•°ç»„è¾“å…¥: shape={image.shape}, ç¬¬ä¸€ä¸ªåƒç´ ={image[0,0]}")
            image_rgb_for_lama = image
        elif isinstance(image, (bytes, io.BytesIO)):
            # å­—èŠ‚è¾“å…¥ - éœ€è¦è§£ç ä¸ºRGB
            logger.info(f"ğŸ’¾ å­—èŠ‚è¾“å…¥ï¼Œç±»å‹: {type(image)}")
            if isinstance(image, io.BytesIO):
                bytes_data = image.getvalue()
            else:
                bytes_data = image
            
            bytes_array = np.asarray(bytearray(bytes_data), dtype=np.uint8)
            image_bgr = cv2.imdecode(bytes_array, cv2.IMREAD_COLOR)
            image_rgb_for_lama = cv2.cvtColor(image_bgr, cv2.COLOR_BGR2RGB)
            logger.info(f"   è§£ç ä¸ºRGB: shape={image_rgb_for_lama.shape}, ç¬¬ä¸€ä¸ªåƒç´ ={image_rgb_for_lama[0,0]}")
        elif hasattr(image, 'size'):
            # PILå›¾åƒ
            logger.info(f"ğŸ–¼ï¸ PILå›¾åƒè¾“å…¥: size={image.size}, mode={image.mode}")
            image_rgb_for_lama = np.array(image.convert("RGB"))
            logger.info(f"   è½¬æ¢ä¸ºRGBæ•°ç»„: shape={image_rgb_for_lama.shape}, ç¬¬ä¸€ä¸ªåƒç´ ={image_rgb_for_lama[0,0]}")
        else:
            # æœªçŸ¥ç±»å‹
            logger.error(f"âŒ ä¸æ”¯æŒçš„å›¾åƒç±»å‹: {type(image)}")
            raise ValueError(f"Unsupported image type: {type(image)}")
        
        # â‘¡ å¤„ç†maskè¾“å…¥
        logger.info("ğŸ­ å¤„ç†mask...")
        logger.info(f"   Maskè¾“å…¥ç±»å‹: {type(mask)}")
        
        if isinstance(mask, np.ndarray):
            logger.info(f"   numpy mask: shape={mask.shape}, dtype={mask.dtype}")
            mask_np = mask if len(mask.shape) == 2 else cv2.cvtColor(mask, cv2.IMREAD_GRAYSCALE)
        else:
            logger.info(f"   PIL mask: size={mask.size}, mode={mask.mode}")
            mask_np = np.array(mask.convert("L"))
        
        logger.info(f"âœ… maskå¤„ç†å®Œæˆ: shape={mask_np.shape}, dtype={mask_np.dtype}")
        logger.info(f"   ç™½è‰²åƒç´ æ•°é‡: {np.sum(mask_np > 128)}")
        logger.info(f"   é»‘è‰²åƒç´ æ•°é‡: {np.sum(mask_np <= 128)}")
        logger.info(f"   maskè¦†ç›–ç‡: {np.sum(mask_np > 128) / mask_np.size * 100:.2f}%")
        logger.info(f"   maskå€¼èŒƒå›´: min={mask_np.min()}, max={mask_np.max()}")
        
        # â‘¢ LaMAå¤„ç† - iopaintè¦æ±‚RGBè¾“å…¥ï¼Œè¾“å‡ºBGR
        logger.info("ğŸ¤– LaMAæ¨¡å‹æ¨ç†ï¼ˆRGBè¾“å…¥â†’BGRè¾“å‡ºï¼‰...")
        logger.info(f"   è¾“å…¥RGBå›¾åƒ: shape={image_rgb_for_lama.shape}, ç¬¬ä¸€ä¸ªåƒç´ ={image_rgb_for_lama[0,0]}")
        logger.info(f"   è¾“å…¥mask: shape={mask_np.shape}, è¦†ç›–ç‡={np.sum(mask_np > 128) / mask_np.size * 100:.2f}%")
        
        # éªŒè¯è¾“å…¥æ•°æ®æœ‰æ•ˆæ€§
        if np.sum(mask_np > 128) == 0:
            logger.warning("âš ï¸ WARNING: Maskä¸­æ²¡æœ‰ç™½è‰²åƒç´ ï¼LaMAå°†ä¸ä¼šè¿›è¡Œä»»ä½•ä¿®å¤ï¼")
        
        # ä¿å­˜è°ƒè¯•ä¿¡æ¯
        logger.info(f"   LaMAé…ç½®éªŒè¯: {config}")
        logger.info(f"   å›¾åƒæ•°æ®èŒƒå›´: min={image_rgb_for_lama.min()}, max={image_rgb_for_lama.max()}")
        logger.info(f"   Maskæ•°æ®èŒƒå›´: min={mask_np.min()}, max={mask_np.max()}")
        
        result_bgr = self.model_manager(image_rgb_for_lama, mask_np, config)
        
        logger.info(f"âœ… LaMAæ¨ç†å®Œæˆ!")
        logger.info(f"   è¾“å‡ºBGRå›¾åƒ: shape={result_bgr.shape}, dtype={result_bgr.dtype}")
        logger.info(f"   è¾“å‡ºç¬¬ä¸€ä¸ªåƒç´ : {result_bgr[0,0]}")
        logger.info(f"   è¾“å‡ºæ•°æ®èŒƒå›´: min={result_bgr.min()}, max={result_bgr.max()}")
        
        if result_bgr.dtype in [np.float64, np.float32]:
            logger.info(f"ğŸ”„ æ•°æ®ç±»å‹è½¬æ¢: {result_bgr.dtype} â†’ uint8")
            result_bgr = np.clip(result_bgr, 0, 255).astype(np.uint8)
            logger.info(f"   è½¬æ¢åæ•°æ®èŒƒå›´: min={result_bgr.min()}, max={result_bgr.max()}")
        
        # â‘£ è½¬æ¢BGRè¾“å‡ºä¸ºRGBç”¨äºPILæ˜¾ç¤ºï¼ˆä¸remwm.pyä¸€è‡´ï¼‰
        logger.info("ğŸ”„ æœ€ç»ˆBGRâ†’RGBè½¬æ¢ï¼ˆä¸remwm.pyä¸€è‡´ï¼‰...")
        result_rgb = cv2.cvtColor(result_bgr, cv2.COLOR_BGR2RGB)
        logger.info(f"   è½¬æ¢åRGBç¬¬ä¸€ä¸ªåƒç´ : {result_rgb[0,0]}")
        
        result_image = Image.fromarray(result_rgb)
        logger.info(f"âœ… æœ€ç»ˆPILå›¾åƒ: size={result_image.size}, mode={result_image.mode}")
        
        return result_image
    
    def get_system_info(self) -> Dict[str, Any]:
        """è·å–ç³»ç»Ÿä¿¡æ¯"""
        import psutil
        
        info = {
            "cuda_available": torch.cuda.is_available(),
            "device": str(self.device),
            "ram_usage": f"{psutil.virtual_memory().percent:.1f}%",
            "cpu_usage": f"{psutil.cpu_percent():.1f}%"
        }
        
        if torch.cuda.is_available():
            gpu_info = torch.cuda.get_device_properties(0)
            vram_total = gpu_info.total_memory // (1024 ** 2)
            vram_used = vram_total - (torch.cuda.memory_reserved(0) // (1024 ** 2))
            info["vram_usage"] = f"{vram_used}/{vram_total} MB"
        else:
            info["vram_usage"] = "N/A"
        
        return info