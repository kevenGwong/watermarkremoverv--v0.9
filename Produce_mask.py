# ----------------- ç¬¬1æ­¥ï¼šå¯¼å…¥åº“ & åŠ è½½æ¨¡åž‹ -----------------
import os
import cv2
import numpy as np
import torch
import matplotlib.pyplot as plt
from PIL import Image
import segmentation_models_pytorch as smp
import albumentations as A
from albumentations.pytorch import ToTensorV2
from typing import List, Tuple

# è®¾ç½®è®¾å¤‡
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# ------------------ å‚æ•°è®¾ç½® ------------------
img_path = "/home/duolaameng/SAM_Remove/WatermarkRemover-AI/IMG_0242-3.jpg"
ckpt_path = "/home/duolaameng/SAM_Remove/WatermarkRemover-AI/data/models/epoch=136-valid_iou=0.8730.ckpt"
output_dir = os.path.dirname(img_path)

# ------------------ ä¸Žè®­ç»ƒæ—¶ç›¸åŒçš„å½’ä¸€åŒ–å‚æ•° ------------------
IMAGENET_MEAN = (0.485, 0.456, 0.406)
IMAGENET_STD = (0.229, 0.224, 0.225)

# ------------------ ä¸Žè®­ç»ƒæ—¶ç›¸åŒçš„éªŒè¯æ•°æ®å¢žå¼ºï¼ˆ512åˆ†è¾¨çŽ‡ï¼‰ ------------------
aug_val = A.Compose([
    A.Normalize(mean=IMAGENET_MEAN, std=IMAGENET_STD, max_pixel_value=255.0),
    ToTensorV2(),
])

# ------------------ å…¨å›¾æŽ¨ç†çš„æ•°æ®å¢žå¼ºï¼ˆresizeåˆ°512x512ï¼‰ ------------------
aug_full = A.Compose([
    A.Resize(512, 512),
    A.Normalize(mean=IMAGENET_MEAN, std=IMAGENET_STD, max_pixel_value=255.0),
    ToTensorV2(),
])

# ----------------- ä¸Žè®­ç»ƒæ—¶ç›¸åŒçš„æ¨¡åž‹å®šä¹‰ -----------------
class WMModel(torch.nn.Module):
    def __init__(self, freeze_encoder=True):
        super().__init__()
        self.net = smp.create_model("FPN", encoder_name="mit_b5", in_channels=3, classes=1)
        
        # åŠ è½½é¢„è®­ç»ƒæƒé‡ï¼ˆå¦‚æžœéœ€è¦çš„è¯ï¼‰
        # PRETRAIN_W = "/home/duolaameng/SAM_Remove/Watermark_sam/watermark-segmentation/best_watermark_model_mit_b5_best.pth"
        # state = torch.load(PRETRAIN_W, map_location="cpu")
        # self.net.load_state_dict(state, strict=False)

        if freeze_encoder:
            for p in self.net.encoder.parameters():
                p.requires_grad = False

    def forward(self, x):
        return self.net(x)

def get_padding(h: int, w: int,
                crop: int = 512,
                step: int = 256) -> Tuple[int, int, int, int]:
    """
    è®¡ç®—åœ¨ (top, bottom, left, right) å››ä¸ªæ–¹å‘å„è¡¥å¤šå°‘åƒç´ ï¼Œ
    ä½¿å¾— padded_hã€padded_w èƒ½ä»¥ `step` æ­¥é•¿å®Œæ•´è¦†ç›–ã€‚
    """
    # ä¿è¯æ»‘çª—èƒ½èµ°åˆ°æœ€å³ / æœ€ä¸‹è€Œä¸æ¬ åƒç´ 
    need_h = (np.ceil((h - crop) / step) * step + crop).astype(int)
    need_w = (np.ceil((w - crop) / step) * step + crop).astype(int)
    pad_h = need_h - h
    pad_w = need_w - w
    # ä¸Šä¸‹å·¦å³å„åˆ†ä¸€åŠï¼ˆå¥‡æ•°æ—¶å¤šè¡¥åˆ° bottom / rightï¼‰
    top    = pad_h // 2
    bottom = pad_h - top
    left   = pad_w // 2
    right  = pad_w - left
    return top, bottom, left, right

def infer_full_image(img_bgr: np.ndarray,
                     model: torch.nn.Module,
                     device: torch.device,
                     crop: int = 512,
                     step: int = 256) -> np.ndarray:
    """
    è¾“å…¥ BGR åŽŸå›¾ï¼Œè¾“å‡ºä¸ŽåŽŸå›¾å°ºå¯¸ä¸€è‡´çš„ float mask (0~1)
    â€”â€” å®Œå…¨ç­‰åŒè®­ç»ƒæ—¶ä»£ç çš„åˆ‡å— / padding é€»è¾‘
    """
    h0, w0 = img_bgr.shape[:2]

    # ---------- â‘  å…ˆæ•´ä½“ padding ----------
    top, bottom, left, right = get_padding(h0, w0, crop, step)
    img_pad = cv2.copyMakeBorder(
        img_bgr, top, bottom, left, right, cv2.BORDER_REFLECT_101)

    H, W = img_pad.shape[:2]
    out_pad = np.zeros((H, W), dtype=np.float32)
    cnt_pad = np.zeros((H, W), dtype=np.float32)

    # ---------- â‘¡ æ»‘çª—åˆ‡å—æŽ¨ç† ----------
    for y in range(0, H - crop + 1, step):
        for x in range(0, W - crop + 1, step):
            patch = img_pad[y:y+crop, x:x+crop]
            sample = aug_val(image=patch)        # ä¸Žè®­ç»ƒåŒæ ·çš„ aug_val
            inp = sample["image"].unsqueeze(0).to(device)

            with torch.no_grad():
                pred = torch.sigmoid(model(inp)).squeeze().cpu().numpy()

            out_pad[y:y+crop, x:x+crop] += pred
            cnt_pad[y:y+crop, x:x+crop] += 1.0

    out_pad /= cnt_pad  # é‡å åŒºåšå¹³å‡

    # ---------- â‘¢ æŠŠ padding è£æŽ‰ï¼Œæ¢å¤åŽŸå°ºå¯¸ ----------
    full_mask = out_pad[top:top+h0, left:left+w0]
    return full_mask

def infer_full_image_less_overlap(img_bgr: np.ndarray,
                                 model: torch.nn.Module,
                                 device: torch.device,
                                 crop: int = 512,
                                 step: int = 384) -> np.ndarray:
    """
    å‡å°‘é‡å é¢ç§¯çš„åˆ‡å—æŽ¨ç†ï¼ˆæ­¥é•¿384ï¼Œé‡å çº¦25%ï¼‰
    """
    h0, w0 = img_bgr.shape[:2]

    # ---------- â‘  å…ˆæ•´ä½“ padding ----------
    top, bottom, left, right = get_padding(h0, w0, crop, step)
    img_pad = cv2.copyMakeBorder(
        img_bgr, top, bottom, left, right, cv2.BORDER_REFLECT_101)

    H, W = img_pad.shape[:2]
    out_pad = np.zeros((H, W), dtype=np.float32)
    cnt_pad = np.zeros((H, W), dtype=np.float32)

    # ---------- â‘¡ æ»‘çª—åˆ‡å—æŽ¨ç†ï¼ˆå‡å°‘é‡å ï¼‰ ----------
    for y in range(0, H - crop + 1, step):
        for x in range(0, W - crop + 1, step):
            patch = img_pad[y:y+crop, x:x+crop]
            sample = aug_val(image=patch)
            inp = sample["image"].unsqueeze(0).to(device)

            with torch.no_grad():
                pred = torch.sigmoid(model(inp)).squeeze().cpu().numpy()

            out_pad[y:y+crop, x:x+crop] += pred
            cnt_pad[y:y+crop, x:x+crop] += 1.0

    out_pad /= cnt_pad  # é‡å åŒºåšå¹³å‡

    # ---------- â‘¢ æŠŠ padding è£æŽ‰ï¼Œæ¢å¤åŽŸå°ºå¯¸ ----------
    full_mask = out_pad[top:top+h0, left:left+w0]
    return full_mask

def infer_full_image_resize(img_bgr: np.ndarray,
                           model: torch.nn.Module,
                           device: torch.device) -> np.ndarray:
    """
    å…¨å›¾resizeåˆ°512x512è¿›è¡ŒæŽ¨ç†ï¼Œç„¶åŽresizeå›žåŽŸå°ºå¯¸
    """
    h0, w0 = img_bgr.shape[:2]
    
    # é¢„å¤„ç†ï¼šresizeåˆ°512x512
    sample = aug_full(image=img_bgr)
    inp = sample["image"].unsqueeze(0).to(device)
    
    # æŽ¨ç†
    with torch.no_grad():
        pred = torch.sigmoid(model(inp)).squeeze().cpu().numpy()
    
    # resizeå›žåŽŸå°ºå¯¸
    pred_resized = cv2.resize(pred, (w0, h0), interpolation=cv2.INTER_LINEAR)
    
    return pred_resized

def count_patches(h: int, w: int, crop: int = 512, step: int = 256) -> int:
    """è®¡ç®—åˆ‡å—æ•°é‡"""
    H = h + (crop - h % step) if h % step != 0 else h
    W = w + (crop - w % step) if w % step != 0 else w
    num_h = (H - crop) // step + 1
    num_w = (W - crop) // step + 1
    return num_h * num_w

# ------------------ åŠ è½½å›¾åƒ ------------------
image = cv2.imread(img_path)
orig_h, orig_w = image.shape[:2]
print(f"ðŸ–¼ï¸ åŽŸå›¾å°ºå¯¸: {orig_w} x {orig_h}")

# ----------------- æž„å»ºæ¨¡åž‹å¹¶åŠ è½½æƒé‡ -----------------
model = WMModel(freeze_encoder=False).to(device)

# åŠ è½½Lightningæƒé‡
ckpt = torch.load(ckpt_path, map_location=device)
state_dict = {k.replace("net.", ""): v for k, v in ckpt["state_dict"].items() if k.startswith("net.")}
model.net.load_state_dict(state_dict)
model.eval()  # å…³é—­Dropoutå’ŒBatchNormçš„è®­ç»ƒè¡Œä¸º
print(f"âœ… æˆåŠŸåŠ è½½æ¨¡åž‹æƒé‡: {ckpt_path}")

# ----------------- è®¡ç®—åˆ‡å—æ•°é‡å¯¹æ¯” -----------------
patches_256 = count_patches(orig_h, orig_w, 512, 256)
patches_384 = count_patches(orig_h, orig_w, 512, 384)
print(f"ðŸ“Š åˆ‡å—æ•°é‡å¯¹æ¯”:")
print(f"   æ­¥é•¿256ï¼ˆ50%é‡å ï¼‰: {patches_256} ä¸ªåˆ‡å—")
print(f"   æ­¥é•¿384ï¼ˆ25%é‡å ï¼‰: {patches_384} ä¸ªåˆ‡å—")
print(f"   å‡å°‘æ¯”ä¾‹: {(patches_256 - patches_384) / patches_256 * 100:.1f}%")

# ----------------- æ–¹æ³•1ï¼šä½¿ç”¨å®Œå…¨ä¸€è‡´çš„åˆ‡å—æŽ¨ç†é€»è¾‘ -----------------
print("\nðŸ” æ–¹æ³•1ï¼šå¼€å§‹åˆ‡å—æŽ¨ç†ï¼ˆæ­¥é•¿256ï¼Œ50%é‡å ï¼‰...")
pred_mask_patch = infer_full_image(image, model, device)
print(f"âœ… åˆ‡å—æŽ¨ç†å®Œæˆï¼Œmaskå°ºå¯¸: {pred_mask_patch.shape}")

# ----------------- æ–¹æ³•2ï¼šå‡å°‘é‡å çš„åˆ‡å—æŽ¨ç† -----------------
print("ðŸ” æ–¹æ³•2ï¼šå¼€å§‹å‡å°‘é‡å åˆ‡å—æŽ¨ç†ï¼ˆæ­¥é•¿384ï¼Œ25%é‡å ï¼‰...")
pred_mask_less = infer_full_image_less_overlap(image, model, device)
print(f"âœ… å‡å°‘é‡å æŽ¨ç†å®Œæˆï¼Œmaskå°ºå¯¸: {pred_mask_less.shape}")

# ----------------- æ–¹æ³•3ï¼šå…¨å›¾resizeæŽ¨ç† -----------------
print("ðŸ” æ–¹æ³•3ï¼šå¼€å§‹å…¨å›¾æŽ¨ç†...")
pred_mask_full = infer_full_image_resize(image, model, device)
print(f"âœ… å…¨å›¾æŽ¨ç†å®Œæˆï¼Œmaskå°ºå¯¸: {pred_mask_full.shape}")

# ----------------- ä¿å­˜ä¸‰ç§æ–¹æ³•çš„ç»“æžœ -----------------
base_name = os.path.splitext(os.path.basename(img_path))[0]

# åˆ‡å—æŽ¨ç†ç»“æžœï¼ˆæ­¥é•¿256ï¼‰
binary_mask_patch = (pred_mask_patch > 0.5).astype(np.uint8) * 255
kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (3, 3))
dilated_mask_patch = cv2.dilate(binary_mask_patch, kernel, iterations=1)

patch_binary_path = os.path.join(output_dir, f"{base_name}_patch_binary_mask.png")
patch_dilated_path = os.path.join(output_dir, f"{base_name}_patch_dilated_mask.png")
patch_raw_path = os.path.join(output_dir, f"{base_name}_patch_raw_mask.png")

cv2.imwrite(patch_binary_path, binary_mask_patch)
cv2.imwrite(patch_dilated_path, dilated_mask_patch)
cv2.imwrite(patch_raw_path, (pred_mask_patch * 255).astype(np.uint8))

# å‡å°‘é‡å åˆ‡å—æŽ¨ç†ç»“æžœï¼ˆæ­¥é•¿384ï¼‰
binary_mask_less = (pred_mask_less > 0.5).astype(np.uint8) * 255
dilated_mask_less = cv2.dilate(binary_mask_less, kernel, iterations=1)

less_binary_path = os.path.join(output_dir, f"{base_name}_less_binary_mask.png")
less_dilated_path = os.path.join(output_dir, f"{base_name}_less_dilated_mask.png")
less_raw_path = os.path.join(output_dir, f"{base_name}_less_raw_mask.png")

cv2.imwrite(less_binary_path, binary_mask_less)
cv2.imwrite(less_dilated_path, dilated_mask_less)
cv2.imwrite(less_raw_path, (pred_mask_less * 255).astype(np.uint8))

# å…¨å›¾æŽ¨ç†ç»“æžœ
binary_mask_full = (pred_mask_full > 0.5).astype(np.uint8) * 255
dilated_mask_full = cv2.dilate(binary_mask_full, kernel, iterations=1)

full_binary_path = os.path.join(output_dir, f"{base_name}_full_binary_mask.png")
full_dilated_path = os.path.join(output_dir, f"{base_name}_full_dilated_mask.png")
full_raw_path = os.path.join(output_dir, f"{base_name}_full_raw_mask.png")

cv2.imwrite(full_binary_path, binary_mask_full)
cv2.imwrite(full_dilated_path, dilated_mask_full)
cv2.imwrite(full_raw_path, (pred_mask_full * 255).astype(np.uint8))

print(f"âœ… åˆ‡å—æŽ¨ç†ç»“æžœä¿å­˜ï¼ˆæ­¥é•¿256ï¼‰:")
print(f"   - {patch_binary_path}")
print(f"   - {patch_dilated_path}")
print(f"   - {patch_raw_path}")
print(f"âœ… å‡å°‘é‡å æŽ¨ç†ç»“æžœä¿å­˜ï¼ˆæ­¥é•¿384ï¼‰:")
print(f"   - {less_binary_path}")
print(f"   - {less_dilated_path}")
print(f"   - {less_raw_path}")
print(f"âœ… å…¨å›¾æŽ¨ç†ç»“æžœä¿å­˜:")
print(f"   - {full_binary_path}")
print(f"   - {full_dilated_path}")
print(f"   - {full_raw_path}")

# ----------------- å¯¹æ¯”æ˜¾ç¤ºç»“æžœ -----------------
plt.figure(figsize=(24, 12))

# åŽŸå›¾
plt.subplot(3, 4, 1)
plt.imshow(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))
plt.title("Original Image")
plt.axis('off')

# åˆ‡å—æŽ¨ç†ç»“æžœï¼ˆæ­¥é•¿256ï¼‰
plt.subplot(3, 4, 2)
plt.imshow(pred_mask_patch, cmap='gray')
plt.title("Patch Inference (step=256) - Raw")
plt.axis('off')

plt.subplot(3, 4, 3)
plt.imshow(binary_mask_patch, cmap='gray')
plt.title("Patch Inference (step=256) - Binary")
plt.axis('off')

plt.subplot(3, 4, 4)
plt.imshow(dilated_mask_patch, cmap='gray')
plt.title("Patch Inference (step=256) - Dilated")
plt.axis('off')

# å‡å°‘é‡å æŽ¨ç†ç»“æžœï¼ˆæ­¥é•¿384ï¼‰
plt.subplot(3, 4, 5)
plt.imshow(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))
plt.title("Original Image")
plt.axis('off')

plt.subplot(3, 4, 6)
plt.imshow(pred_mask_less, cmap='gray')
plt.title("Patch Inference (step=384) - Raw")
plt.axis('off')

plt.subplot(3, 4, 7)
plt.imshow(binary_mask_less, cmap='gray')
plt.title("Patch Inference (step=384) - Binary")
plt.axis('off')

plt.subplot(3, 4, 8)
plt.imshow(dilated_mask_less, cmap='gray')
plt.title("Patch Inference (step=384) - Dilated")
plt.axis('off')

# å…¨å›¾æŽ¨ç†ç»“æžœ
plt.subplot(3, 4, 9)
plt.imshow(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))
plt.title("Original Image")
plt.axis('off')

plt.subplot(3, 4, 10)
plt.imshow(pred_mask_full, cmap='gray')
plt.title("Full Image Inference - Raw")
plt.axis('off')

plt.subplot(3, 4, 11)
plt.imshow(binary_mask_full, cmap='gray')
plt.title("Full Image Inference - Binary")
plt.axis('off')

plt.subplot(3, 4, 12)
plt.imshow(dilated_mask_full, cmap='gray')
plt.title("Full Image Inference - Dilated")
plt.axis('off')

plt.tight_layout()
plt.savefig(os.path.join(output_dir, f"{base_name}_three_methods_comparison.png"), dpi=150, bbox_inches='tight')
plt.show()

# ----------------- è®¡ç®—å·®å¼‚ç»Ÿè®¡ -----------------
diff_mask_256_384 = np.abs(pred_mask_patch - pred_mask_less)
diff_binary_256_384 = np.abs(binary_mask_patch - binary_mask_less)

diff_mask_256_full = np.abs(pred_mask_patch - pred_mask_full)
diff_binary_256_full = np.abs(binary_mask_patch - binary_mask_full)

print(f"\nðŸ“Š ä¸‰ç§æ–¹æ³•å·®å¼‚ç»Ÿè®¡:")
print(f"   æ­¥é•¿256 vs æ­¥é•¿384:")
print(f"     åŽŸå§‹maskå¹³å‡å·®å¼‚: {np.mean(diff_mask_256_384):.4f}")
print(f"     äºŒå€¼åŒ–maskå·®å¼‚åƒç´ æ•°: {np.sum(diff_binary_256_384 > 0)}")
print(f"     äºŒå€¼åŒ–maskå·®å¼‚æ¯”ä¾‹: {np.sum(diff_binary_256_384 > 0) / (orig_h * orig_w) * 100:.2f}%")
print(f"   æ­¥é•¿256 vs å…¨å›¾æŽ¨ç†:")
print(f"     åŽŸå§‹maskå¹³å‡å·®å¼‚: {np.mean(diff_mask_256_full):.4f}")
print(f"     äºŒå€¼åŒ–maskå·®å¼‚åƒç´ æ•°: {np.sum(diff_binary_256_full > 0)}")
print(f"     äºŒå€¼åŒ–maskå·®å¼‚æ¯”ä¾‹: {np.sum(diff_binary_256_full > 0) / (orig_h * orig_w) * 100:.2f}%")
