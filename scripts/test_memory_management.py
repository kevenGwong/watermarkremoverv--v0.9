#!/usr/bin/env python3
"""
CUDAå†…å­˜ç®¡ç†æµ‹è¯•è„šæœ¬
éªŒè¯SimplifiedWatermarkProcessorçš„å†…å­˜ç®¡ç†åŠŸèƒ½
"""

import sys
import os
import time
import torch
from pathlib import Path
from PIL import Image
import numpy as np
import logging

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
sys.path.insert(0, str(Path(__file__).parent))

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

def test_memory_management():
    """æµ‹è¯•å†…å­˜ç®¡ç†åŠŸèƒ½"""
    logger.info("ğŸ§ª å¼€å§‹CUDAå†…å­˜ç®¡ç†æµ‹è¯•...")
    
    # æ£€æŸ¥CUDAå¯ç”¨æ€§
    if not torch.cuda.is_available():
        logger.error("âŒ CUDAä¸å¯ç”¨ï¼Œè·³è¿‡å†…å­˜ç®¡ç†æµ‹è¯•")
        return False
    
    try:
        # å¯¼å…¥å¿…è¦çš„æ¨¡å—
        from core.processors.simplified_watermark_processor import SimplifiedWatermarkProcessor
        from core.utils.memory_monitor import MemoryMonitor
        
        # åˆ›å»ºæµ‹è¯•é…ç½®
        config = {
            'device': 'cuda',
            'mask_generator': {},
            'iopaint_config': {
                'device': 'cuda',
                'hd_strategy': 'Original',
                'hd_strategy_crop_margin': 128,
                'hd_strategy_crop_trigger_size': 512,
                'hd_strategy_resize_limit': 2048
            }
        }
        
        # åˆ›å»ºæµ‹è¯•å›¾åƒ
        test_image = Image.new('RGB', (512, 512), color='red')
        test_mask = Image.new('L', (512, 512), color=255)
        
        # åˆå§‹åŒ–å¤„ç†å™¨
        logger.info("ğŸ”§ åˆå§‹åŒ–SimplifiedWatermarkProcessor...")
        processor = SimplifiedWatermarkProcessor(config)
        
        # åˆå§‹å†…å­˜çŠ¶æ€
        memory_monitor = MemoryMonitor()
        initial_memory = memory_monitor.get_memory_info()
        logger.info(f"ğŸ“Š åˆå§‹å†…å­˜çŠ¶æ€: {initial_memory}")
        
        # æµ‹è¯•æ¨¡å‹åˆ‡æ¢å’Œå†…å­˜ç®¡ç†
        models_to_test = ["mat", "zits", "fcf"]
        
        for model_name in models_to_test:
            logger.info(f"\nğŸ”„ æµ‹è¯•æ¨¡å‹åˆ‡æ¢: {model_name}")
            
            try:
                # è®°å½•åˆ‡æ¢å‰å†…å­˜
                before_memory = memory_monitor.get_memory_info()
                logger.info(f"ğŸ“Š åˆ‡æ¢å‰å†…å­˜: {before_memory}")
                
                # æ¨¡æ‹Ÿå¤„ç†ï¼ˆä¸å®é™…æ‰§è¡Œæ¨ç†ï¼Œåªæµ‹è¯•æ¨¡å‹åŠ è½½ï¼‰
                logger.info(f"âš¡ æ¨¡æ‹Ÿå¤„ç† {model_name} æ¨¡å‹...")
                
                # åˆ‡æ¢æ¨¡å‹
                processor._switch_model(model_name)
                
                # è®°å½•åˆ‡æ¢åå†…å­˜
                after_memory = memory_monitor.get_memory_info()
                logger.info(f"ğŸ“Š åˆ‡æ¢åå†…å­˜: {after_memory}")
                
                # éªŒè¯æ¨¡å‹çŠ¶æ€
                status = processor.get_model_status()
                logger.info(f"âœ… æ¨¡å‹çŠ¶æ€: {status}")
                
                # ç­‰å¾…ä¸€ä¸‹è®©å†…å­˜ç¨³å®š
                time.sleep(2)
                
            except Exception as e:
                logger.error(f"âŒ æ¨¡å‹ {model_name} æµ‹è¯•å¤±è´¥: {e}")
                continue
        
        # æµ‹è¯•èµ„æºæ¸…ç†
        logger.info("\nğŸ§¹ æµ‹è¯•èµ„æºæ¸…ç†...")
        before_cleanup = memory_monitor.get_memory_info()
        
        processor.cleanup()
        
        after_cleanup = memory_monitor.get_memory_info()
        logger.info(f"ğŸ“Š æ¸…ç†å‰å†…å­˜: {before_cleanup}")
        logger.info(f"ğŸ“Š æ¸…ç†åå†…å­˜: {after_cleanup}")
        
        # éªŒè¯å†…å­˜é‡Šæ”¾
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
            final_memory = memory_monitor.get_memory_info()
            logger.info(f"ğŸ“Š æœ€ç»ˆå†…å­˜çŠ¶æ€: {final_memory}")
        
        logger.info("âœ… å†…å­˜ç®¡ç†æµ‹è¯•å®Œæˆï¼")
        return True
        
    except Exception as e:
        logger.error(f"âŒ å†…å­˜ç®¡ç†æµ‹è¯•å¤±è´¥: {e}")
        return False

def test_memory_pressure():
    """æµ‹è¯•å†…å­˜å‹åŠ›æƒ…å†µ"""
    logger.info("ğŸ”¥ å¼€å§‹å†…å­˜å‹åŠ›æµ‹è¯•...")
    
    if not torch.cuda.is_available():
        logger.error("âŒ CUDAä¸å¯ç”¨ï¼Œè·³è¿‡å‹åŠ›æµ‹è¯•")
        return False
    
    try:
        from core.utils.memory_monitor import MemoryMonitor
        
        memory_monitor = MemoryMonitor()
        
        # åˆ†é…å¤§é‡å†…å­˜æµ‹è¯•
        logger.info("ğŸ“ˆ åˆ†é…æµ‹è¯•å†…å­˜...")
        test_tensors = []
        
        for i in range(5):
            # åˆ†é…100MBçš„å¼ é‡
            tensor = torch.randn(100, 1024, 1024, device='cuda')
            test_tensors.append(tensor)
            
            memory_info = memory_monitor.get_memory_info()
            logger.info(f"ğŸ“Š åˆ†é…ç¬¬{i+1}ä¸ªå¼ é‡åå†…å­˜: {memory_info}")
            
            # æ£€æŸ¥æ˜¯å¦æ¥è¿‘å†…å­˜é™åˆ¶
            if memory_info.get('gpu_memory_percent', 0) > 80:
                logger.warning("âš ï¸ GPUå†…å­˜ä½¿ç”¨ç‡è¶…è¿‡80%ï¼Œåœæ­¢åˆ†é…")
                break
        
        # æ¸…ç†æµ‹è¯•å†…å­˜
        logger.info("ğŸ§¹ æ¸…ç†æµ‹è¯•å†…å­˜...")
        del test_tensors
        torch.cuda.empty_cache()
        
        final_memory = memory_monitor.get_memory_info()
        logger.info(f"ğŸ“Š æ¸…ç†åå†…å­˜: {final_memory}")
        
        logger.info("âœ… å†…å­˜å‹åŠ›æµ‹è¯•å®Œæˆï¼")
        return True
        
    except Exception as e:
        logger.error(f"âŒ å†…å­˜å‹åŠ›æµ‹è¯•å¤±è´¥: {e}")
        return False

def main():
    """ä¸»æµ‹è¯•å‡½æ•°"""
    logger.info("ğŸš€ å¼€å§‹å†…å­˜ç®¡ç†æµ‹è¯•å¥—ä»¶...")
    
    # æµ‹è¯•1: åŸºæœ¬å†…å­˜ç®¡ç†
    test1_result = test_memory_management()
    
    # æµ‹è¯•2: å†…å­˜å‹åŠ›æµ‹è¯•
    test2_result = test_memory_pressure()
    
    # æ±‡æ€»ç»“æœ
    if test1_result and test2_result:
        logger.info("ğŸ‰ æ‰€æœ‰å†…å­˜ç®¡ç†æµ‹è¯•é€šè¿‡ï¼")
        return 0
    else:
        logger.error("âŒ éƒ¨åˆ†æµ‹è¯•å¤±è´¥ï¼Œè¯·æ£€æŸ¥æ—¥å¿—")
        return 1

if __name__ == "__main__":
    sys.exit(main())