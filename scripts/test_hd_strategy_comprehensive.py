#!/usr/bin/env python3
"""
HD Strategy é«˜æ¸…å¤„ç†ç­–ç•¥å…¨é¢æµ‹è¯•è„šæœ¬
éªŒè¯ORIGINALã€CROPã€RESIZEä¸‰ç§æ¨¡å¼çš„æ­£ç¡®æ€§
"""

import os
import sys
import time
import json
import numpy as np
from PIL import Image, ImageDraw, ImageFont
import logging
from pathlib import Path
from typing import Dict, List, Tuple, Any
from dataclasses import dataclass

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

from core.inference import process_image
from config.config import ConfigManager
from core.utils.image_utils import ImageValidator

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

@dataclass
class TestResult:
    """æµ‹è¯•ç»“æœæ•°æ®ç±»"""
    test_name: str
    original_size: Tuple[int, int]
    result_size: Tuple[int, int]
    hd_strategy: str
    processing_time: float
    success: bool
    size_preserved: bool
    quality_score: float
    error_message: str = ""
    
    def to_dict(self) -> Dict[str, Any]:
        return {
            'test_name': self.test_name,
            'original_size': self.original_size,
            'result_size': self.result_size,
            'hd_strategy': self.hd_strategy,
            'processing_time': self.processing_time,
            'success': self.success,
            'size_preserved': self.size_preserved,
            'quality_score': self.quality_score,
            'error_message': self.error_message
        }

class HDStrategyTester:
    """HD Strategyæµ‹è¯•å™¨"""
    
    def __init__(self):
        self.config_manager = ConfigManager()
        self.test_results: List[TestResult] = []
        self.output_dir = Path("scripts/hd_strategy_test_results")
        self.output_dir.mkdir(exist_ok=True)
        
        # æµ‹è¯•ç­–ç•¥é…ç½®
        self.strategies = ['ORIGINAL', 'CROP', 'RESIZE']
        self.test_sizes = [
            (512, 512),      # å°å°ºå¯¸
            (800, 600),      # ä¸­ç­‰å°ºå¯¸
            (1024, 768),     # æ ‡å‡†åˆ†è¾¨ç‡
            (1280, 720),     # 720p
            (1920, 1080),    # 1080p
            (2048, 1536),    # 2K
            (2560, 1440),    # 1440p
            (3840, 2160)     # 4K
        ]
        
        # æµ‹è¯•æ¨¡å‹ï¼ˆé€‰æ‹©å¤„ç†é€Ÿåº¦è¾ƒå¿«çš„ï¼‰
        self.test_models = ['fcf', 'lama']
        
    def create_test_image_with_watermark(self, size: Tuple[int, int]) -> Tuple[Image.Image, Image.Image]:
        """åˆ›å»ºå¸¦æ°´å°çš„æµ‹è¯•å›¾åƒå’Œå¯¹åº”çš„mask"""
        width, height = size
        
        # åˆ›å»ºå¤æ‚çš„æµ‹è¯•å›¾åƒ
        image = Image.new('RGB', size, color=(240, 240, 240))
        draw = ImageDraw.Draw(image)
        
        # æ·»åŠ æ¸å˜èƒŒæ™¯
        for y in range(height):
            color_val = int(200 + 55 * (y / height))
            draw.line([(0, y), (width, y)], fill=(color_val, color_val - 20, color_val - 40))
        
        # æ·»åŠ å‡ ä½•å›¾å½¢
        draw.rectangle([width//4, height//4, 3*width//4, 3*height//4], 
                      fill=(100, 150, 200), outline=(50, 100, 150), width=3)
        
        # æ·»åŠ åœ†å½¢
        circle_size = min(width, height) // 8
        draw.ellipse([width//2 - circle_size, height//2 - circle_size,
                     width//2 + circle_size, height//2 + circle_size],
                    fill=(255, 100, 100), outline=(200, 50, 50), width=2)
        
        # æ·»åŠ æ–‡æœ¬
        try:
            font = ImageFont.load_default()
            text = f"Test Image {width}x{height}"
            draw.text((20, 20), text, fill=(0, 0, 0), font=font)
        except:
            # å¦‚æœå­—ä½“åŠ è½½å¤±è´¥ï¼Œä½¿ç”¨é»˜è®¤å­—ä½“
            draw.text((20, 20), f"Test {width}x{height}", fill=(0, 0, 0))
        
        # åˆ›å»ºæ°´å°åŒºåŸŸï¼ˆå³ä¸‹è§’ï¼‰
        watermark_w, watermark_h = width // 6, height // 8
        watermark_x = width - watermark_w - 20
        watermark_y = height - watermark_h - 20
        
        # åŠé€æ˜æ°´å°
        watermark = Image.new('RGBA', (watermark_w, watermark_h), (255, 255, 255, 128))
        watermark_draw = ImageDraw.Draw(watermark)
        watermark_draw.rectangle([0, 0, watermark_w-1, watermark_h-1], 
                               fill=(200, 200, 200, 180), outline=(100, 100, 100, 255))
        try:
            watermark_draw.text((10, 10), "WATERMARK", fill=(0, 0, 0, 200))
        except:
            pass
        
        # ç²˜è´´æ°´å°
        image.paste(watermark, (watermark_x, watermark_y), watermark)
        
        # åˆ›å»ºå¯¹åº”çš„mask
        mask = Image.new('L', size, color=0)
        mask_draw = ImageDraw.Draw(mask)
        # æ°´å°åŒºåŸŸè®¾ä¸ºç™½è‰²
        mask_draw.rectangle([watermark_x, watermark_y, 
                           watermark_x + watermark_w, watermark_y + watermark_h],
                          fill=255)
        
        return image, mask
    
    def calculate_image_quality_score(self, original: Image.Image, result: Image.Image) -> float:
        """è®¡ç®—å›¾åƒè´¨é‡åˆ†æ•°ï¼ˆåŸºäºPSNRï¼‰"""
        try:
            # ç¡®ä¿ä¸¤ä¸ªå›¾åƒå°ºå¯¸ç›¸åŒ
            if original.size != result.size:
                result = result.resize(original.size, Image.LANCZOS)
            
            # è½¬æ¢ä¸ºnumpyæ•°ç»„
            orig_array = np.array(original.convert('RGB'))
            result_array = np.array(result.convert('RGB'))
            
            # è®¡ç®—MSE
            mse = np.mean((orig_array - result_array) ** 2)
            
            # é¿å…é™¤é›¶é”™è¯¯
            if mse == 0:
                return 100.0
            
            # è®¡ç®—PSNR
            psnr = 20 * np.log10(255.0 / np.sqrt(mse))
            
            # å½’ä¸€åŒ–åˆ°0-100åˆ†æ•°
            quality_score = min(100.0, max(0.0, psnr))
            
            return quality_score
            
        except Exception as e:
            logger.warning(f"æ— æ³•è®¡ç®—å›¾åƒè´¨é‡åˆ†æ•°: {e}")
            return 0.0
    
    def test_single_configuration(self, 
                                 size: Tuple[int, int], 
                                 strategy: str, 
                                 model: str) -> TestResult:
        """æµ‹è¯•å•ä¸€é…ç½®"""
        test_name = f"{size[0]}x{size[1]}_{strategy}_{model}"
        logger.info(f"ğŸ§ª æµ‹è¯•é…ç½®: {test_name}")
        
        # åˆ›å»ºæµ‹è¯•æ•°æ®
        original_image, test_mask = self.create_test_image_with_watermark(size)
        
        # ä¿å­˜åŸå§‹å›¾åƒï¼ˆç”¨äºå¯¹æ¯”ï¼‰
        original_path = self.output_dir / f"{test_name}_original.png"
        original_image.save(original_path)
        
        # ä¿å­˜mask
        mask_path = self.output_dir / f"{test_name}_mask.png"
        test_mask.save(mask_path)
        
        # è®¾ç½®å¤„ç†å‚æ•°
        mask_params = {
            'uploaded_mask': test_mask,
            'mask_dilate_kernel_size': 1,
            'mask_dilate_iterations': 1
        }
        
        # æ ¹æ®ç­–ç•¥è®¾ç½®å‚æ•°
        if strategy == 'ORIGINAL':
            crop_trigger = 99999  # è¶³å¤Ÿå¤§ï¼Œæ°¸è¿œä¸ä¼šè§¦å‘
            resize_limit = 99999
        elif strategy == 'CROP':
            crop_trigger = 800    # è¾ƒå°çš„è§¦å‘å°ºå¯¸
            resize_limit = 2048
        else:  # RESIZE
            crop_trigger = 99999  # ä¸è§¦å‘crop
            resize_limit = 1024   # è¾ƒå°çš„resizeé™åˆ¶
        
        inpaint_params = {
            'inpaint_model': 'iopaint',
            'force_model': model,
            'auto_model_selection': False,
            'ldm_steps': 20,
            'hd_strategy': strategy,
            'hd_strategy_crop_margin': 64,
            'hd_strategy_crop_trigger_size': crop_trigger,
            'hd_strategy_resize_limit': resize_limit,
            'seed': 42  # å›ºå®šéšæœºç§å­
        }
        
        performance_params = {
            'mixed_precision': True,
            'log_processing_time': True
        }
        
        # æ‰§è¡Œå¤„ç†
        start_time = time.time()
        
        try:
            result = process_image(
                image=original_image,
                mask_model='upload',
                mask_params=mask_params,
                inpaint_params=inpaint_params,
                performance_params=performance_params,
                transparent=False,
                config_manager=self.config_manager
            )
            
            end_time = time.time()
            processing_time = end_time - start_time
            
            if result.success:
                # ä¿å­˜ç»“æœ
                result_path = self.output_dir / f"{test_name}_result.png"
                result.result_image.save(result_path)
                
                # æ£€æŸ¥å°ºå¯¸ä¿æŒ
                size_preserved = result.result_image.size == original_image.size
                
                # è®¡ç®—è´¨é‡åˆ†æ•°
                quality_score = self.calculate_image_quality_score(original_image, result.result_image)
                
                test_result = TestResult(
                    test_name=test_name,
                    original_size=original_image.size,
                    result_size=result.result_image.size,
                    hd_strategy=strategy,
                    processing_time=processing_time,
                    success=True,
                    size_preserved=size_preserved,
                    quality_score=quality_score
                )
                
                logger.info(f"âœ… {test_name} æˆåŠŸ - å°ºå¯¸ä¿æŒ: {size_preserved}, è´¨é‡åˆ†æ•°: {quality_score:.2f}")
                
            else:
                test_result = TestResult(
                    test_name=test_name,
                    original_size=original_image.size,
                    result_size=(0, 0),
                    hd_strategy=strategy,
                    processing_time=processing_time,
                    success=False,
                    size_preserved=False,
                    quality_score=0.0,
                    error_message=result.error_message
                )
                
                logger.error(f"âŒ {test_name} å¤±è´¥: {result.error_message}")
                
        except Exception as e:
            end_time = time.time()
            processing_time = end_time - start_time
            
            test_result = TestResult(
                test_name=test_name,
                original_size=original_image.size,
                result_size=(0, 0),
                hd_strategy=strategy,
                processing_time=processing_time,
                success=False,
                size_preserved=False,
                quality_score=0.0,
                error_message=str(e)
            )
            
            logger.error(f"âŒ {test_name} å¼‚å¸¸: {str(e)}")
        
        return test_result
    
    def run_comprehensive_tests(self) -> Dict[str, Any]:
        """è¿è¡Œå…¨é¢çš„HDç­–ç•¥æµ‹è¯•"""
        logger.info("ğŸš€ å¼€å§‹HDç­–ç•¥å…¨é¢æµ‹è¯•")
        logger.info(f"æµ‹è¯•é…ç½®: {len(self.test_sizes)} ç§å°ºå¯¸ Ã— {len(self.strategies)} ç§ç­–ç•¥ Ã— {len(self.test_models)} ç§æ¨¡å‹")
        
        # åˆ›å»ºæµ‹è¯•çŸ©é˜µ
        test_configurations = []
        for size in self.test_sizes:
            for strategy in self.strategies:
                for model in self.test_models:
                    test_configurations.append((size, strategy, model))
        
        logger.info(f"æ€»æµ‹è¯•æ•°é‡: {len(test_configurations)}")
        
        # æ‰§è¡Œæµ‹è¯•
        for i, (size, strategy, model) in enumerate(test_configurations, 1):
            logger.info(f"\nğŸ“Š è¿›åº¦: {i}/{len(test_configurations)}")
            
            test_result = self.test_single_configuration(size, strategy, model)
            self.test_results.append(test_result)
            
            # å†…å­˜æ¸…ç†
            if i % 10 == 0:
                logger.info("ğŸ§¹ æ‰§è¡Œå†…å­˜æ¸…ç†...")
                try:
                    import gc
                    gc.collect()
                    if 'torch' in sys.modules:
                        import torch
                        if torch.cuda.is_available():
                            torch.cuda.empty_cache()
                except:
                    pass
        
        # åˆ†æç»“æœ
        return self.analyze_results()
    
    def analyze_results(self) -> Dict[str, Any]:
        """åˆ†ææµ‹è¯•ç»“æœ"""
        logger.info("\nğŸ“ˆ åˆ†ææµ‹è¯•ç»“æœ...")
        
        total_tests = len(self.test_results)
        successful_tests = sum(1 for r in self.test_results if r.success)
        
        # æŒ‰ç­–ç•¥åˆ†ç»„åˆ†æ
        strategy_stats = {}
        for strategy in self.strategies:
            strategy_results = [r for r in self.test_results if r.hd_strategy == strategy]
            successful_strategy = [r for r in strategy_results if r.success]
            
            if successful_strategy:
                avg_time = sum(r.processing_time for r in successful_strategy) / len(successful_strategy)
                avg_quality = sum(r.quality_score for r in successful_strategy) / len(successful_strategy)
                size_preserved_rate = sum(1 for r in successful_strategy if r.size_preserved) / len(successful_strategy)
            else:
                avg_time = 0
                avg_quality = 0
                size_preserved_rate = 0
            
            strategy_stats[strategy] = {
                'total_tests': len(strategy_results),
                'successful_tests': len(successful_strategy),
                'success_rate': len(successful_strategy) / len(strategy_results) if strategy_results else 0,
                'avg_processing_time': avg_time,
                'avg_quality_score': avg_quality,
                'size_preserved_rate': size_preserved_rate
            }
        
        # æŒ‰å°ºå¯¸åˆ†ç»„åˆ†æ
        size_stats = {}
        for size in self.test_sizes:
            size_results = [r for r in self.test_results if r.original_size == size]
            successful_size = [r for r in size_results if r.success]
            
            if successful_size:
                avg_time = sum(r.processing_time for r in successful_size) / len(successful_size)
                avg_quality = sum(r.quality_score for r in successful_size) / len(successful_size)
                size_preserved_rate = sum(1 for r in successful_size if r.size_preserved) / len(successful_size)
            else:
                avg_time = 0
                avg_quality = 0
                size_preserved_rate = 0
            
            size_stats[f"{size[0]}x{size[1]}"] = {
                'total_tests': len(size_results),
                'successful_tests': len(successful_size),
                'success_rate': len(successful_size) / len(size_results) if size_results else 0,
                'avg_processing_time': avg_time,
                'avg_quality_score': avg_quality,
                'size_preserved_rate': size_preserved_rate
            }
        
        # é—®é¢˜åˆ†æ
        failed_tests = [r for r in self.test_results if not r.success]
        size_changed_tests = [r for r in self.test_results if r.success and not r.size_preserved]
        
        analysis = {
            'total_tests': total_tests,
            'successful_tests': successful_tests,
            'success_rate': successful_tests / total_tests if total_tests > 0 else 0,
            'strategy_stats': strategy_stats,
            'size_stats': size_stats,
            'failed_tests': len(failed_tests),
            'size_changed_tests': len(size_changed_tests),
            'test_results': [r.to_dict() for r in self.test_results]
        }
        
        # ä¿å­˜åˆ†æç»“æœ
        analysis_path = self.output_dir / "analysis_results.json"
        with open(analysis_path, 'w', encoding='utf-8') as f:
            json.dump(analysis, f, indent=2, ensure_ascii=False)
        
        logger.info(f"ğŸ“Š åˆ†æç»“æœå·²ä¿å­˜åˆ°: {analysis_path}")
        
        return analysis
    
    def generate_report(self, analysis: Dict[str, Any]) -> str:
        """ç”Ÿæˆæµ‹è¯•æŠ¥å‘Š"""
        logger.info("ğŸ“ ç”Ÿæˆæµ‹è¯•æŠ¥å‘Š...")
        
        report = []
        report.append("=" * 80)
        report.append("HD Strategy é«˜æ¸…å¤„ç†ç­–ç•¥æµ‹è¯•æŠ¥å‘Š")
        report.append("=" * 80)
        report.append(f"æµ‹è¯•æ—¶é—´: {time.strftime('%Y-%m-%d %H:%M:%S')}")
        report.append(f"æµ‹è¯•é…ç½®: {len(self.test_sizes)} ç§å°ºå¯¸ Ã— {len(self.strategies)} ç§ç­–ç•¥ Ã— {len(self.test_models)} ç§æ¨¡å‹")
        report.append("")
        
        # æ€»ä½“ç»Ÿè®¡
        report.append("ğŸ“Š æ€»ä½“ç»Ÿè®¡")
        report.append("-" * 40)
        report.append(f"æ€»æµ‹è¯•æ•°: {analysis['total_tests']}")
        report.append(f"æˆåŠŸæµ‹è¯•æ•°: {analysis['successful_tests']}")
        report.append(f"æˆåŠŸç‡: {analysis['success_rate']:.2%}")
        report.append(f"å¤±è´¥æµ‹è¯•æ•°: {analysis['failed_tests']}")
        report.append(f"å°ºå¯¸å˜åŒ–æµ‹è¯•æ•°: {analysis['size_changed_tests']}")
        report.append("")
        
        # ç­–ç•¥å¯¹æ¯”
        report.append("ğŸ¯ HDç­–ç•¥å¯¹æ¯”")
        report.append("-" * 40)
        for strategy, stats in analysis['strategy_stats'].items():
            report.append(f"{strategy} ç­–ç•¥:")
            report.append(f"  æˆåŠŸç‡: {stats['success_rate']:.2%} ({stats['successful_tests']}/{stats['total_tests']})")
            report.append(f"  å¹³å‡å¤„ç†æ—¶é—´: {stats['avg_processing_time']:.2f}ç§’")
            report.append(f"  å¹³å‡è´¨é‡åˆ†æ•°: {stats['avg_quality_score']:.2f}")
            report.append(f"  å°ºå¯¸ä¿æŒç‡: {stats['size_preserved_rate']:.2%}")
            report.append("")
        
        # å°ºå¯¸åˆ†æ
        report.append("ğŸ“ å°ºå¯¸åˆ†æ")
        report.append("-" * 40)
        for size_key, stats in analysis['size_stats'].items():
            report.append(f"{size_key}:")
            report.append(f"  æˆåŠŸç‡: {stats['success_rate']:.2%} ({stats['successful_tests']}/{stats['total_tests']})")
            report.append(f"  å¹³å‡å¤„ç†æ—¶é—´: {stats['avg_processing_time']:.2f}ç§’")
            report.append(f"  å¹³å‡è´¨é‡åˆ†æ•°: {stats['avg_quality_score']:.2f}")
            report.append(f"  å°ºå¯¸ä¿æŒç‡: {stats['size_preserved_rate']:.2%}")
            report.append("")
        
        # é—®é¢˜åˆ†æ
        if analysis['failed_tests'] > 0:
            report.append("âŒ å¤±è´¥æµ‹è¯•åˆ†æ")
            report.append("-" * 40)
            failed_results = [r for r in self.test_results if not r.success]
            for result in failed_results:
                report.append(f"  {result.test_name}: {result.error_message}")
            report.append("")
        
        if analysis['size_changed_tests'] > 0:
            report.append("âš ï¸  å°ºå¯¸å˜åŒ–æµ‹è¯•åˆ†æ")
            report.append("-" * 40)
            size_changed_results = [r for r in self.test_results if r.success and not r.size_preserved]
            for result in size_changed_results:
                report.append(f"  {result.test_name}: {result.original_size} -> {result.result_size}")
            report.append("")
        
        # å»ºè®®
        report.append("ğŸ’¡ å»ºè®®")
        report.append("-" * 40)
        
        # åˆ†ææœ€ä½³ç­–ç•¥
        best_strategy = max(analysis['strategy_stats'].items(), 
                          key=lambda x: x[1]['success_rate'] * x[1]['size_preserved_rate'])
        report.append(f"æœ€ä½³ç­–ç•¥: {best_strategy[0]} (æˆåŠŸç‡: {best_strategy[1]['success_rate']:.2%}, å°ºå¯¸ä¿æŒ: {best_strategy[1]['size_preserved_rate']:.2%})")
        
        # ORIGINALç­–ç•¥åˆ†æ
        original_stats = analysis['strategy_stats'].get('ORIGINAL', {})
        if original_stats.get('size_preserved_rate', 0) < 1.0:
            report.append("âš ï¸  ORIGINALç­–ç•¥æœªèƒ½100%ä¿æŒå°ºå¯¸ï¼Œéœ€è¦æ£€æŸ¥å®ç°")
        
        # CROPç­–ç•¥åˆ†æ
        crop_stats = analysis['strategy_stats'].get('CROP', {})
        if crop_stats.get('success_rate', 0) < 0.9:
            report.append("âš ï¸  CROPç­–ç•¥æˆåŠŸç‡åä½ï¼Œéœ€è¦ä¼˜åŒ–åˆ†å—é€»è¾‘")
        
        # RESIZEç­–ç•¥åˆ†æ
        resize_stats = analysis['strategy_stats'].get('RESIZE', {})
        if resize_stats.get('size_preserved_rate', 0) > 0.1:
            report.append("âš ï¸  RESIZEç­–ç•¥åº”è¯¥ä¼šæ”¹å˜å°ºå¯¸ï¼Œä½†éƒ¨åˆ†æµ‹è¯•ä¿æŒäº†åŸå°ºå¯¸")
        
        report.append("")
        report.append("=" * 80)
        
        report_content = "\n".join(report)
        
        # ä¿å­˜æŠ¥å‘Š
        report_path = self.output_dir / "test_report.txt"
        with open(report_path, 'w', encoding='utf-8') as f:
            f.write(report_content)
        
        logger.info(f"ğŸ“„ æµ‹è¯•æŠ¥å‘Šå·²ä¿å­˜åˆ°: {report_path}")
        
        return report_content

def main():
    """ä¸»å‡½æ•°"""
    logger.info("ğŸ” HD Strategy é«˜æ¸…å¤„ç†ç­–ç•¥æµ‹è¯•å¼€å§‹")
    
    # æ£€æŸ¥CUDAå¯ç”¨æ€§
    try:
        import torch
        if torch.cuda.is_available():
            logger.info(f"âœ… CUDAå¯ç”¨ï¼ŒGPU: {torch.cuda.get_device_name()}")
        else:
            logger.warning("âš ï¸  CUDAä¸å¯ç”¨ï¼Œå°†ä½¿ç”¨CPUå¤„ç†")
    except ImportError:
        logger.warning("âš ï¸  PyTorchæœªå®‰è£…ï¼ŒæŸäº›åŠŸèƒ½å¯èƒ½å—é™")
    
    # åˆ›å»ºæµ‹è¯•å™¨
    tester = HDStrategyTester()
    
    # è¿è¡Œæµ‹è¯•
    try:
        analysis = tester.run_comprehensive_tests()
        
        # ç”ŸæˆæŠ¥å‘Š
        report = tester.generate_report(analysis)
        
        # æ‰“å°æŠ¥å‘Š
        print("\n" + report)
        
        # åˆ¤æ–­æµ‹è¯•æ˜¯å¦é€šè¿‡
        success_rate = analysis['success_rate']
        size_preservation_rate = analysis['strategy_stats'].get('ORIGINAL', {}).get('size_preserved_rate', 0)
        
        if success_rate >= 0.9 and size_preservation_rate >= 0.95:
            logger.info("ğŸ‰ HD Strategyæµ‹è¯•é€šè¿‡ï¼")
            return True
        else:
            logger.warning("âš ï¸  HD Strategyæµ‹è¯•æœªå®Œå…¨é€šè¿‡ï¼Œéœ€è¦è¿›ä¸€æ­¥ä¼˜åŒ–")
            return False
            
    except Exception as e:
        logger.error(f"âŒ æµ‹è¯•è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")
        return False

if __name__ == "__main__":
    success = main()
    sys.exit(0 if success else 1)