# WatermarkRemover-AI é¡¹ç›®ä¿®å¤æ–¹æ¡ˆ
**æ—¶é—´**: 12:20 PM 7æœˆ5æ—¥  
**ç¯å¢ƒ**: conda py310aiwatermark  
**ç›®æ ‡**: è§£å†³ä¸»ç¨‹åºå¯åŠ¨å¤±è´¥é—®é¢˜ï¼Œç¡®ä¿IOPainté›†æˆæ­£å¸¸å·¥ä½œ

---

## ğŸ“Š å½“å‰ç¯å¢ƒçŠ¶æ€æ£€æŸ¥

### âœ… å·²å®‰è£…çš„ä¾èµ–
- **Python**: 3.10.18
- **PyTorch**: 2.7.1+cu126 (CUDAå¯ç”¨)
- **OpenCV**: 4.11.0.86
- **Albumentations**: 2.0.8
- **Segmentation Models PyTorch**: 0.5.0
- **Transformers**: 4.48.3

### âŒ ç¼ºå¤±çš„å…³é”®ä¾èµ–
- **IOPaint**: æœªå®‰è£…
- **saicinpainting**: æœªå®‰è£…

### âœ… å·²å­˜åœ¨çš„æ¨¡å‹æ–‡ä»¶
- **è‡ªå®šä¹‰Maskæ¨¡å‹**: `data/models/epoch=071-valid_iou=0.7267.ckpt` (1GB)
- **IOPaintç¼“å­˜æ¨¡å‹**:
  - `big-lama.pt` (LaMAæ¨¡å‹)
  - `zits-*.pt` (ZITSæ¨¡å‹ç›¸å…³æ–‡ä»¶)
  - `Places_512_*.pth` (MATæ¨¡å‹ç›¸å…³æ–‡ä»¶)

---

## ğŸ¯ é—®é¢˜è¯Šæ–­æ€»ç»“

### ğŸ” æ ¸å¿ƒé—®é¢˜
1. **ConfigManagerä¼ é€’å¤±è´¥** - `'NoneType' object has no attribute 'get_config'`
2. **IOPaintä¾èµ–ç¼ºå¤±** - æ¨¡å‹åŠ è½½å¤±è´¥ï¼Œéœ€è¦å®‰è£…å®˜æ–¹ä¾èµ–
3. **saicinpaintingç¼ºå¤±** - LaMAæ¨¡å‹æ— æ³•æ­£å¸¸å·¥ä½œ
4. **ä¸»ç¨‹åºåˆå§‹åŒ–é”™è¯¯** - `main.py`ä¸­ConfigManagerå®ä¾‹åŒ–é—®é¢˜

### ğŸ¯ ä¿®å¤ç›®æ ‡
- ç¡®ä¿Streamlit WebUIæ­£å¸¸å¯åŠ¨
- æˆåŠŸåŠ è½½ZITS/MAT/FCF/LaMAå››ç§æ¨¡å‹
- å®ç°å®Œæ•´çš„æ°´å°å»é™¤å·¥ä½œæµ
- è¾¾åˆ°ç”Ÿäº§å°±ç»ªçŠ¶æ€

---

## ğŸ“ è¯¦ç»†ä¿®å¤æ–¹æ¡ˆ

### ç¬¬ä¸€é˜¶æ®µï¼šç¼ºå¤±ä¾èµ–å®‰è£… (é¢„è®¡20åˆ†é’Ÿ)

#### 1.1 å®‰è£…IOPaintå®˜æ–¹åŒ…
```bash
# æ¿€æ´»ç¯å¢ƒ
conda activate py310aiwatermark

# å®‰è£…IOPaint (åŸºäºå®˜æ–¹GitHub)
pip install iopaint

# éªŒè¯å®‰è£…
python -c "import iopaint; print('IOPaint version:', iopaint.__version__)"
```

#### 1.2 å®‰è£…saicinpainting (LaMAæ¨¡å‹å¿…éœ€)
```bash
# æ–¹æ³•1: ç›´æ¥å®‰è£…
pip install saicinpainting

# æ–¹æ³•2: å¦‚æœä¸Šè¿°å¤±è´¥ï¼Œä»æºç å®‰è£…
git clone https://github.com/saic-mdal/lama.git
cd lama
pip install -e .
cd ..
rm -rf lama
```

#### 1.3 éªŒè¯æ‰€æœ‰ä¾èµ–
```bash
# åˆ›å»ºä¾èµ–éªŒè¯è„šæœ¬
python -c "
import torch
import iopaint
import saicinpainting
import segmentation_models_pytorch
import transformers
import albumentations
import cv2
print('âœ… æ‰€æœ‰ä¾èµ–å®‰è£…æˆåŠŸ')
print(f'PyTorch: {torch.__version__}')
print(f'CUDA: {torch.cuda.is_available()}')
"
```

### ç¬¬äºŒé˜¶æ®µï¼šæ¨¡å‹æ–‡ä»¶éªŒè¯ (é¢„è®¡15åˆ†é’Ÿ)

#### 2.1 æ£€æŸ¥IOPaintæ¨¡å‹ç¼“å­˜
```bash
# æ£€æŸ¥IOPaintæ¨¡å‹ç¼“å­˜ç›®å½•
ls -la ~/.cache/torch/hub/checkpoints/

# éªŒè¯å…³é”®æ¨¡å‹æ–‡ä»¶
python -c "
import os
cache_dir = os.path.expanduser('~/.cache/torch/hub/checkpoints/')
models = ['big-lama.pt', 'zits-inpaint-0717.pt', 'Places_512_FullData_G.pth']
for model in models:
    path = os.path.join(cache_dir, model)
    if os.path.exists(path):
        size = os.path.getsize(path) / (1024*1024)
        print(f'âœ… {model}: {size:.1f}MB')
    else:
        print(f'âŒ {model}: ç¼ºå¤±')
"
```

#### 2.2 éªŒè¯è‡ªå®šä¹‰Maskæ¨¡å‹
```bash
# æ£€æŸ¥è‡ªå®šä¹‰æ¨¡å‹æ–‡ä»¶
ls -la data/models/epoch=071-valid_iou=0.7267.ckpt

# éªŒè¯æ¨¡å‹æ–‡ä»¶å®Œæ•´æ€§
python -c "
import torch
try:
    ckpt = torch.load('data/models/epoch=071-valid_iou=0.7267.ckpt', map_location='cpu')
    print('âœ… è‡ªå®šä¹‰Maskæ¨¡å‹æ–‡ä»¶å®Œæ•´')
    print(f'   çŠ¶æ€å­—å…¸é”®æ•°é‡: {len(ckpt.get(\"state_dict\", {}))}')
except Exception as e:
    print(f'âŒ è‡ªå®šä¹‰Maskæ¨¡å‹æ–‡ä»¶æŸå: {e}')
"
```

### ç¬¬ä¸‰é˜¶æ®µï¼šä»£ç ä¿®å¤ (é¢„è®¡60åˆ†é’Ÿ)

#### 3.1 ä¿®å¤main.pyåˆå§‹åŒ–é—®é¢˜
```python
# interfaces/web/main.py ä¿®å¤
import streamlit as st
import logging
from pathlib import Path

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO)

# å¯¼å…¥æ¨¡å—
from config.config import ConfigManager
from interfaces.web.ui import MainInterface
from core.inference import get_inference_manager, get_system_info

# æ­£ç¡®åˆå§‹åŒ–ConfigManager
config_manager = ConfigManager("web_config.yaml")

# ç¡®ä¿ä¼ é€’config_managerå®ä¾‹
def main():
    try:
        # è·å–æ¨ç†ç®¡ç†å™¨
        inference_manager = get_inference_manager(config_manager)
        if inference_manager is None:
            st.error("âŒ Failed to initialize inference manager")
            return
        
        # è·å–ç³»ç»Ÿä¿¡æ¯
        system_info = get_system_info(config_manager)
        
        # åˆå§‹åŒ–ä¸»ç•Œé¢
        main_interface = MainInterface(config_manager)
        
        # æ¸²æŸ“ç•Œé¢
        main_interface.render(inference_manager)
        
    except Exception as e:
        st.error(f"âŒ Application startup failed: {e}")
        logging.error(f"Startup error: {e}")

if __name__ == "__main__":
    main()
```

#### 3.2 ä¿®å¤InferenceManageré”™è¯¯å¤„ç†
```python
# core/inference_manager.py ä¿®å¤
def __init__(self, config_manager, config_path: Optional[str] = None):
    if config_manager is None:
        raise ValueError("ConfigManager cannot be None")
    
    self.config_manager = config_manager
    self.config_path = config_path
    
    # åˆå§‹åŒ–ç»Ÿä¸€å¤„ç†å™¨
    self.unified_processor = None
    
    # åˆå§‹åŒ–maskç”Ÿæˆå™¨
    self.custom_mask_generator = None
    self.florence_mask_generator = None
    self.fallback_mask_generator = None
    
    logger.info("âœ… InferenceManager initialized with config_manager")
```

#### 3.3 ä¿®å¤UnifiedProcessoræ¨¡å‹åŠ è½½
```python
# core/models/unified_processor.py ä¿®å¤
def _load_processors(self):
    """åŠ è½½æ‰€æœ‰å¤„ç†å™¨ï¼Œå¢åŠ é”™è¯¯æ¢å¤æœºåˆ¶"""
    loaded_count = 0
    errors = []
    
    # æŒ‰ä¼˜å…ˆçº§åŠ è½½æ¨¡å‹ (MAT > FCF > LaMA > ZITS)
    model_priority = ["mat", "fcf", "lama", "zits"]
    
    for model_name in model_priority:
        try:
            if model_name == "mat":
                self.processors["mat"] = MatProcessor(self.config)
            elif model_name == "fcf":
                self.processors["fcf"] = FcfProcessor(self.config)
            elif model_name == "lama":
                self.processors["lama"] = LamaProcessor(self.config)
            elif model_name == "zits":
                self.processors["zits"] = ZitsProcessor(self.config)
            
            loaded_count += 1
            logger.info(f"âœ… {model_name.upper()} processor loaded")
            
        except Exception as e:
            error_msg = f"Failed to load {model_name.upper()}: {e}"
            errors.append(error_msg)
            logger.warning(f"âš ï¸ {error_msg}")
            continue
    
    if loaded_count == 0:
        error_summary = "\n".join(errors)
        raise RuntimeError(f"No models could be loaded:\n{error_summary}")
    
    # è®¾ç½®é»˜è®¤å¤„ç†å™¨
    self.current_processor = list(self.processors.keys())[0]
    logger.info(f"âœ… Unified processor initialized with {loaded_count}/{len(model_priority)} models")
    logger.info(f"   Available: {list(self.processors.keys())}")
    logger.info(f"   Default: {self.current_processor}")
```

### ç¬¬å››é˜¶æ®µï¼šé…ç½®æ–‡ä»¶ä¼˜åŒ– (é¢„è®¡30åˆ†é’Ÿ)

#### 4.1 åˆ›å»ºå®Œæ•´çš„web_config.yaml
```yaml
# web_config.yaml å®Œæ•´é…ç½®
app:
  title: "AI Watermark Remover - IOPaint Edition"
  host: "0.0.0.0"
  port: 8501
  debug: true

# IOPaintæ¨¡å‹é…ç½®
models:
  iopaint_models_dir: "~/.cache/torch/hub/checkpoints"
  default_inpainting: "mat"
  available_models: ["zits", "mat", "fcf", "lama"]
  
  # æ¨¡å‹è·¯å¾„é…ç½®
  lama_model: "lama"
  mat_model: "mat"
  zits_model: "zits"
  fcf_model: "fcf"

# Maskç”Ÿæˆå™¨é…ç½®
mask_generator:
  model_type: "custom"
  mask_model_path: "./data/models/epoch=071-valid_iou=0.7267.ckpt"
  image_size: 768
  imagenet_mean: [0.485, 0.456, 0.406]
  imagenet_std: [0.229, 0.224, 0.225]
  mask_threshold: 0.5
  mask_dilate_kernel_size: 3
  mask_dilate_iterations: 1

# IOPaintå¤„ç†é…ç½®
iopaint_config:
  hd_strategy: "CROP"
  hd_strategy_crop_margin: 64
  hd_strategy_crop_trigger_size: 1024
  hd_strategy_resize_limit: 2048
  ldm_steps: 50
  ldm_sampler: "ddim"
  auto_model_selection: true

# Florence-2é…ç½®
florence:
  model_name: "microsoft/Florence-2-large"
  trust_remote_code: true
  prompt: "watermark"
  task: "object_detection"
  enabled: true

# æ–‡ä»¶å¤„ç†é…ç½®
files:
  max_upload_size: 20
  temp_dir: "./temp"
  output_dir: "./data/output"
  allowed_extensions: [".jpg", ".jpeg", ".png", ".webp"]

# UIé…ç½®
ui:
  mask_generation_options: ["custom", "florence", "upload"]
  show_model_selector: true
  show_advanced_options: true
  show_auto_selection: true
  default_download_format: "png"

# æ€§èƒ½é…ç½®
performance:
  log_processing_time: true
  log_memory_usage: true
  clear_cache_after_processing: true

# æ—¥å¿—é…ç½®
logging:
  level: "INFO"
  format: "%(asctime)s | %(levelname)-8s | %(name)s:%(funcName)s:%(lineno)d - %(message)s"
```

#### 4.2 åˆ›å»ºç¯å¢ƒæ£€æŸ¥è„šæœ¬
```python
# scripts/check_environment.py
import os
import sys
import importlib
from pathlib import Path

def check_environment():
    """æ£€æŸ¥ç¯å¢ƒä¾èµ–å’Œæ¨¡å‹æ–‡ä»¶"""
    print("ğŸ” ç¯å¢ƒæ£€æŸ¥å¼€å§‹...")
    
    checks = {
        "python_version": check_python_version(),
        "torch": check_torch(),
        "iopaint": check_iopaint(),
        "saicinpainting": check_saicinpainting(),
        "other_deps": check_other_dependencies(),
        "models": check_model_files(),
        "config": check_config_files()
    }
    
    # è¾“å‡ºæ£€æŸ¥ç»“æœ
    print("\nğŸ“Š æ£€æŸ¥ç»“æœæ±‡æ€»:")
    for check_name, result in checks.items():
        status = "âœ…" if result else "âŒ"
        print(f"{status} {check_name}")
    
    return all(checks.values())

def check_python_version():
    """æ£€æŸ¥Pythonç‰ˆæœ¬"""
    version = sys.version_info
    if version.major == 3 and version.minor >= 10:
        print(f"âœ… Pythonç‰ˆæœ¬: {version.major}.{version.minor}.{version.micro}")
        return True
    else:
        print(f"âŒ Pythonç‰ˆæœ¬è¿‡ä½: {version.major}.{version.minor}.{version.micro}")
        return False

def check_torch():
    """æ£€æŸ¥PyTorch"""
    try:
        import torch
        print(f"âœ… PyTorch: {torch.__version__}")
        print(f"   CUDAå¯ç”¨: {torch.cuda.is_available()}")
        if torch.cuda.is_available():
            print(f"   GPU: {torch.cuda.get_device_name()}")
        return True
    except ImportError:
        print("âŒ PyTorchæœªå®‰è£…")
        return False

def check_iopaint():
    """æ£€æŸ¥IOPaint"""
    try:
        import iopaint
        print(f"âœ… IOPaint: {iopaint.__version__}")
        return True
    except ImportError:
        print("âŒ IOPaintæœªå®‰è£…")
        return False

def check_saicinpainting():
    """æ£€æŸ¥saicinpainting"""
    try:
        import saicinpainting
        print("âœ… saicinpaintingå¯ç”¨")
        return True
    except ImportError:
        print("âŒ saicinpaintingæœªå®‰è£…")
        return False

def check_other_dependencies():
    """æ£€æŸ¥å…¶ä»–ä¾èµ–"""
    deps = [
        "segmentation_models_pytorch",
        "albumentations", 
        "transformers",
        "cv2",
        "PIL"
    ]
    
    all_ok = True
    for dep in deps:
        try:
            importlib.import_module(dep)
            print(f"âœ… {dep}")
        except ImportError:
            print(f"âŒ {dep}")
            all_ok = False
    
    return all_ok

def check_model_files():
    """æ£€æŸ¥æ¨¡å‹æ–‡ä»¶"""
    print("\nğŸ“ æ¨¡å‹æ–‡ä»¶æ£€æŸ¥:")
    
    # æ£€æŸ¥è‡ªå®šä¹‰Maskæ¨¡å‹
    mask_path = Path("data/models/epoch=071-valid_iou=0.7267.ckpt")
    if mask_path.exists():
        size_mb = mask_path.stat().st_size / (1024*1024)
        print(f"âœ… è‡ªå®šä¹‰Maskæ¨¡å‹: {size_mb:.1f}MB")
    else:
        print("âŒ è‡ªå®šä¹‰Maskæ¨¡å‹ç¼ºå¤±")
        return False
    
    # æ£€æŸ¥IOPaintæ¨¡å‹ç¼“å­˜
    cache_dir = Path.home() / ".cache/torch/hub/checkpoints"
    if cache_dir.exists():
        models = ["big-lama.pt", "zits-inpaint-0717.pt", "Places_512_FullData_G.pth"]
        found_models = []
        for model in models:
            if (cache_dir / model).exists():
                found_models.append(model)
        
        if found_models:
            print(f"âœ… IOPaintæ¨¡å‹ç¼“å­˜: {len(found_models)}ä¸ªæ¨¡å‹")
            for model in found_models:
                size_mb = (cache_dir / model).stat().st_size / (1024*1024)
                print(f"   - {model}: {size_mb:.1f}MB")
        else:
            print("âŒ IOPaintæ¨¡å‹ç¼“å­˜ä¸ºç©º")
            return False
    else:
        print("âŒ IOPaintæ¨¡å‹ç¼“å­˜ç›®å½•ä¸å­˜åœ¨")
        return False
    
    return True

def check_config_files():
    """æ£€æŸ¥é…ç½®æ–‡ä»¶"""
    print("\nâš™ï¸ é…ç½®æ–‡ä»¶æ£€æŸ¥:")
    
    config_files = ["web_config.yaml", "config/config.py"]
    all_ok = True
    
    for config_file in config_files:
        if Path(config_file).exists():
            print(f"âœ… {config_file}")
        else:
            print(f"âŒ {config_file}")
            all_ok = False
    
    return all_ok

if __name__ == "__main__":
    success = check_environment()
    if success:
        print("\nğŸ‰ ç¯å¢ƒæ£€æŸ¥é€šè¿‡ï¼å¯ä»¥å¯åŠ¨åº”ç”¨ã€‚")
    else:
        print("\nâš ï¸ ç¯å¢ƒæ£€æŸ¥å¤±è´¥ï¼Œè¯·æŒ‰ä¿®å¤æ–¹æ¡ˆè§£å†³ã€‚")
```

### ç¬¬äº”é˜¶æ®µï¼šæµ‹è¯•éªŒè¯ (é¢„è®¡45åˆ†é’Ÿ)

#### 5.1 åˆ›å»ºå¯åŠ¨æµ‹è¯•è„šæœ¬
```python
# scripts/test_startup.py
import sys
import logging
from pathlib import Path

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

def test_startup():
    """æµ‹è¯•ç¨‹åºå¯åŠ¨æµç¨‹"""
    print("ğŸš€ å¯åŠ¨æµ‹è¯•å¼€å§‹...")
    
    try:
        # 1. æµ‹è¯•ConfigManageråˆå§‹åŒ–
        print("1. æµ‹è¯•ConfigManageråˆå§‹åŒ–...")
        from config.config import ConfigManager
        config_manager = ConfigManager("web_config.yaml")
        print("âœ… ConfigManager initialized")
        
        # 2. æµ‹è¯•InferenceManageråˆå§‹åŒ–
        print("2. æµ‹è¯•InferenceManageråˆå§‹åŒ–...")
        from core.inference import get_inference_manager
        inference_manager = get_inference_manager(config_manager)
        if inference_manager is None:
            raise RuntimeError("InferenceManager returned None")
        print("âœ… InferenceManager initialized")
        
        # 3. æµ‹è¯•æ¨¡å‹åŠ è½½
        print("3. æµ‹è¯•æ¨¡å‹åŠ è½½...")
        available_models = inference_manager.get_available_models()
        print(f"âœ… Available models: {available_models}")
        
        # 4. æµ‹è¯•UIåˆå§‹åŒ–
        print("4. æµ‹è¯•UIåˆå§‹åŒ–...")
        from interfaces.web.ui import MainInterface
        main_interface = MainInterface(config_manager)
        print("âœ… MainInterface initialized")
        
        # 5. æµ‹è¯•ç³»ç»Ÿä¿¡æ¯è·å–
        print("5. æµ‹è¯•ç³»ç»Ÿä¿¡æ¯è·å–...")
        from core.inference import get_system_info
        system_info = get_system_info(config_manager)
        print(f"âœ… System info: {system_info}")
        
        print("\nğŸ‰ å¯åŠ¨æµ‹è¯•å…¨éƒ¨é€šè¿‡ï¼")
        return True
        
    except Exception as e:
        print(f"\nâŒ å¯åŠ¨æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False

if __name__ == "__main__":
    success = test_startup()
    sys.exit(0 if success else 1)
```

#### 5.2 åˆ›å»ºåŠŸèƒ½æµ‹è¯•è„šæœ¬
```python
# scripts/test_functionality.py
import sys
import numpy as np
from PIL import Image
from pathlib import Path

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

def test_basic_functionality():
    """æµ‹è¯•åŸºæœ¬åŠŸèƒ½"""
    print("ğŸ§ª åŠŸèƒ½æµ‹è¯•å¼€å§‹...")
    
    try:
        from config.config import ConfigManager
        from core.inference import get_inference_manager, process_image
        
        # 1. åˆå§‹åŒ–
        config_manager = ConfigManager("web_config.yaml")
        inference_manager = get_inference_manager(config_manager)
        
        # 2. åˆ›å»ºæµ‹è¯•å›¾åƒ
        print("1. åˆ›å»ºæµ‹è¯•å›¾åƒ...")
        test_image = Image.new('RGB', (512, 512), 'red')
        print("âœ… æµ‹è¯•å›¾åƒåˆ›å»ºæˆåŠŸ")
        
        # 3. æµ‹è¯•maskç”Ÿæˆ
        print("2. æµ‹è¯•maskç”Ÿæˆ...")
        mask_params = {
            'mask_threshold': 0.5,
            'mask_dilate_kernel_size': 3,
            'mask_dilate_iterations': 1
        }
        print("âœ… Maskå‚æ•°è®¾ç½®æˆåŠŸ")
        
        # 4. æµ‹è¯•inpaintingå‚æ•°
        print("3. æµ‹è¯•inpaintingå‚æ•°...")
        inpaint_params = {
            'force_model': 'mat',
            'ldm_steps': 20,
            'hd_strategy': 'CROP'
        }
        print("âœ… Inpaintingå‚æ•°è®¾ç½®æˆåŠŸ")
        
        # 5. æµ‹è¯•å›¾åƒå¤„ç†
        print("4. æµ‹è¯•å›¾åƒå¤„ç†...")
        result = process_image(
            image=test_image,
            mask_model='custom',
            mask_params=mask_params,
            inpaint_params=inpaint_params,
            performance_params={},
            transparent=False,
            config_manager=config_manager
        )
        
        if result.success:
            print("âœ… å›¾åƒå¤„ç†æˆåŠŸ")
            print(f"   å¤„ç†æ—¶é—´: {result.processing_time:.2f}ç§’")
            if result.result_image:
                print(f"   ç»“æœå›¾åƒå°ºå¯¸: {result.result_image.size}")
            if result.mask_image:
                print(f"   Maskå›¾åƒå°ºå¯¸: {result.mask_image.size}")
        else:
            print(f"âŒ å›¾åƒå¤„ç†å¤±è´¥: {result.error_message}")
            return False
        
        print("\nğŸ‰ åŠŸèƒ½æµ‹è¯•å…¨éƒ¨é€šè¿‡ï¼")
        return True
        
    except Exception as e:
        print(f"\nâŒ åŠŸèƒ½æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False

if __name__ == "__main__":
    success = test_basic_functionality()
    sys.exit(0 if success else 1)
```

---

## ğŸ¯ æ‰§è¡Œè®¡åˆ’

### æ—¶é—´å®‰æ’
- **12:20-12:40** - ç¼ºå¤±ä¾èµ–å®‰è£… (IOPaint + saicinpainting)
- **12:40-12:55** - æ¨¡å‹æ–‡ä»¶éªŒè¯
- **12:55-13:55** - ä»£ç ä¿®å¤ (main.py + inference_manager.py + unified_processor.py)
- **13:55-14:25** - é…ç½®æ–‡ä»¶ä¼˜åŒ– (web_config.yaml + æ£€æŸ¥è„šæœ¬)
- **14:25-15:10** - æµ‹è¯•éªŒè¯ (å¯åŠ¨æµ‹è¯• + åŠŸèƒ½æµ‹è¯•)

### æˆåŠŸæ ‡å‡†
1. âœ… `streamlit run interfaces/web/main.py --server.port 8501` æ­£å¸¸å¯åŠ¨
2. âœ… WebUIç•Œé¢å®Œæ•´æ˜¾ç¤ºï¼Œæ— é”™è¯¯ä¿¡æ¯
3. âœ… è‡³å°‘2ä¸ªæ¨¡å‹æˆåŠŸåŠ è½½ï¼ˆMATã€FCFï¼‰
4. âœ… å®Œæ•´çš„å›¾åƒå¤„ç†æµç¨‹æµ‹è¯•é€šè¿‡
5. âœ… å¤„ç†ç»“æœè´¨é‡ç¬¦åˆé¢„æœŸ

### é£é™©æ§åˆ¶
- æ¯ä¸ªé˜¶æ®µå®Œæˆåç«‹å³æµ‹è¯•
- ä¿ç•™åŸå§‹ä»£ç å¤‡ä»½
- å‡†å¤‡å›æ»šæ–¹æ¡ˆ
- è®°å½•è¯¦ç»†çš„é”™è¯¯æ—¥å¿—

---

## ğŸ“‹ æ£€æŸ¥æ¸…å•

### ç¯å¢ƒæ£€æŸ¥
- [ ] condaç¯å¢ƒæ¿€æ´»æ­£ç¡® (py310aiwatermark)
- [ ] IOPaintåŒ…å®‰è£…æˆåŠŸ
- [ ] saicinpaintingå®‰è£…æˆåŠŸ
- [ ] å…¶ä»–ä¾èµ–åŒ…éªŒè¯é€šè¿‡

### æ¨¡å‹æ£€æŸ¥
- [ ] IOPaintæ¨¡å‹ç¼“å­˜éªŒè¯é€šè¿‡
- [ ] è‡ªå®šä¹‰maskæ¨¡å‹æ–‡ä»¶éªŒè¯é€šè¿‡
- [ ] æ¨¡å‹è·¯å¾„é…ç½®æ­£ç¡®
- [ ] æ¨¡å‹åŠ è½½æµ‹è¯•é€šè¿‡

### ä»£ç æ£€æŸ¥
- [ ] main.py ConfigManageråˆå§‹åŒ–ä¿®å¤
- [ ] InferenceManageré”™è¯¯å¤„ç†ä¿®å¤
- [ ] UnifiedProcessoræ¨¡å‹åŠ è½½ä¿®å¤
- [ ] UIç»„ä»¶åˆå§‹åŒ–ä¿®å¤

### é…ç½®æ£€æŸ¥
- [ ] web_config.yamlé…ç½®å®Œæ•´
- [ ] æ¨¡å‹è·¯å¾„æ˜ å°„æ­£ç¡®
- [ ] å‚æ•°éªŒè¯é€»è¾‘æ­£ç¡®
- [ ] é»˜è®¤å€¼è®¾ç½®åˆç†

### æµ‹è¯•æ£€æŸ¥
- [ ] ç¯å¢ƒæ£€æŸ¥è„šæœ¬é€šè¿‡
- [ ] ç¨‹åºå¯åŠ¨æµ‹è¯•é€šè¿‡
- [ ] æ¨¡å‹åŠ è½½æµ‹è¯•é€šè¿‡
- [ ] åŸºæœ¬åŠŸèƒ½æµ‹è¯•é€šè¿‡

---

## ğŸ”§ å¿«é€Ÿä¿®å¤å‘½ä»¤

### ä¸€é”®å®‰è£…ç¼ºå¤±ä¾èµ–
```bash
conda activate py310aiwatermark
pip install iopaint saicinpainting
```

### ä¸€é”®ç¯å¢ƒæ£€æŸ¥
```bash
python scripts/check_environment.py
```

### ä¸€é”®å¯åŠ¨æµ‹è¯•
```bash
python scripts/test_startup.py
```

### ä¸€é”®åŠŸèƒ½æµ‹è¯•
```bash
python scripts/test_functionality.py
```

### ä¸€é”®å¯åŠ¨åº”ç”¨
```bash
streamlit run interfaces/web/main.py --server.port 8501
```

---

## ğŸ“š å‚è€ƒèµ„æº

- [IOPaintå®˜æ–¹æ–‡æ¡£](https://www.iopaint.com/)
- [IOPaint GitHubä»“åº“](https://github.com/Sanster/IOPaint)
- [LaMAé¡¹ç›®](https://github.com/saic-mdal/lama)
- [PyTorchå®˜æ–¹æ–‡æ¡£](https://pytorch.org/)

---

**å¤‡æ³¨**: æœ¬ä¿®å¤æ–¹æ¡ˆåŸºäºå½“å‰ç¯å¢ƒçŠ¶æ€åˆ¶å®šï¼Œå……åˆ†åˆ©ç”¨å·²æœ‰çš„æ¨¡å‹æ–‡ä»¶å’Œä¾èµ–ï¼Œé¿å…é‡å¤ä¸‹è½½å’Œå®‰è£…ã€‚ä¿®å¤å®Œæˆåï¼Œé¡¹ç›®å°†è¾¾åˆ°ç”Ÿäº§å°±ç»ªçŠ¶æ€ã€‚ 

## ä¿®å¤æ¦‚è¿°
æœ¬æ–‡æ¡£æä¾›äº†WatermarkRemover-AIé¡¹ç›®çš„ç³»ç»Ÿæ€§ä¿®å¤æ–¹æ¡ˆï¼Œæ¶µç›–æ¶æ„ä¼˜åŒ–ã€æ€§èƒ½æå‡å’ŒåŠŸèƒ½å®Œå–„ã€‚

## 1. æ ¸å¿ƒé—®é¢˜ä¿®å¤

### 1.1 CUDAå†…å­˜ç®¡ç†ä¼˜åŒ– ğŸ”´ é«˜ä¼˜å…ˆçº§

#### é—®é¢˜æè¿°
- æ¨¡å‹åˆ‡æ¢æ—¶æ˜¾å­˜æœªæ¸…ç†ï¼Œå¯¼è‡´OOM
- å¤šä¸ªæ¨¡å‹åŒæ—¶åŠ è½½å ç”¨å¤§é‡æ˜¾å­˜
- ç¼ºä¹æ™ºèƒ½æ˜¾å­˜ç®¡ç†ç­–ç•¥

#### ä¿®å¤æ–¹æ¡ˆ

**1.1.1 å®ç°æ™ºèƒ½æ¨¡å‹å¸è½½æœºåˆ¶**
```python
# åœ¨ core/models/unified_processor.py ä¸­æ·»åŠ 
def switch_model(self, model_name: str) -> bool:
    """æ™ºèƒ½åˆ‡æ¢æ¨¡å‹ï¼Œè‡ªåŠ¨æ¸…ç†æ˜¾å­˜"""
    if model_name == self.current_processor:
        return True
        
    try:
        # æ¸…ç†å½“å‰æ¨¡å‹
        if self.current_processor and self.current_processor in self.processors:
            self.processors[self.current_processor].cleanup_resources()
            del self.processors[self.current_processor]
            
        # æ¸…ç†CUDAç¼“å­˜
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
            
        # åŠ è½½æ–°æ¨¡å‹
        self._load_specific_processor(model_name)
        self.current_processor = model_name
        
        return True
    except Exception as e:
        logger.error(f"Model switch failed: {e}")
        return False
```

**1.1.2 æ·»åŠ æ˜¾å­˜ç›‘æ§**
```python
# åœ¨ core/utils/memory_monitor.py ä¸­å®ç°
class MemoryMonitor:
    @staticmethod
    def get_gpu_memory_info():
        if torch.cuda.is_available():
            allocated = torch.cuda.memory_allocated() / 1024**3
            reserved = torch.cuda.memory_reserved() / 1024**3
            total = torch.cuda.get_device_properties(0).total_memory / 1024**3
            return {
                'allocated_gb': allocated,
                'reserved_gb': reserved,
                'total_gb': total,
                'free_gb': total - reserved
            }
        return None
```

**1.1.3 å®ç°æ¨¡å‹æ‡’åŠ è½½**
```python
# ä¿®æ”¹ core/models/unified_processor.py
def _load_processors(self):
    """æ‡’åŠ è½½å¤„ç†å™¨ï¼ŒåªåŠ è½½å½“å‰éœ€è¦çš„æ¨¡å‹"""
    # åªåŠ è½½é»˜è®¤æ¨¡å‹ï¼Œå…¶ä»–æ¨¡å‹æŒ‰éœ€åŠ è½½
    default_model = self.config.get('default_model', 'zits')
    self._load_specific_processor(default_model)
    self.current_processor = default_model
```

#### é¢„æœŸæ•ˆæœ
- æ˜¾å­˜ä½¿ç”¨å‡å°‘50%ä»¥ä¸Š
- æ¨¡å‹åˆ‡æ¢æ—¶é—´ < 5ç§’
- æ”¯æŒæ›´å¤§å›¾åƒå¤„ç†

### 1.2 LaMAæ¨¡å‹æ¨¡å—åŒ–ä¿®å¤ ğŸŸ¡ ä¸­ä¼˜å…ˆçº§

#### é—®é¢˜æè¿°
- `saicinpainting`ä¾èµ–ç¼ºå¤±
- LaMAå¤„ç†å™¨ç¼ºä¹é™çº§æœºåˆ¶
- éœ€è¦å‚è€ƒå¼€æºé¡¹ç›®å®ç°

#### ä¿®å¤æ–¹æ¡ˆ

**1.2.1 å‚è€ƒå¼€æºå®ç°**
å‚è€ƒ [kevenGwong/watermarkremoverv--v0.9](https://github.com/kevenGwong/watermarkremoverv--v0.9/blob/v1.0-refactored/watermark_remover_ai/core/processors/watermark_processor.py) çš„æ¨¡å—åŒ–è®¾è®¡

**1.2.2 å®ç°å¯é€‰LaMAæ”¯æŒ**
```python
# åœ¨ core/models/lama_processor.py ä¸­æ·»åŠ 
class OptionalLamaProcessor:
    def __init__(self, config):
        self.available = self._check_dependencies()
        if self.available:
            self.processor = LamaProcessor(config)
        else:
            self.processor = None
            
    def _check_dependencies(self):
        try:
            import saicinpainting
            return True
        except ImportError:
            logger.warning("saicinpainting not available, LaMA disabled")
            return False
            
    def predict(self, image, mask, config=None):
        if not self.available:
            raise RuntimeError("LaMA not available")
        return self.processor.predict(image, mask, config)
```

**1.2.3 æ·»åŠ ä¾èµ–å®‰è£…è„šæœ¬**
```bash
# scripts/install_lama_deps.sh
#!/bin/bash
pip install saicinpainting
# æˆ–è€…ä½¿ç”¨conda
conda install -c conda-forge saicinpainting
```

#### é¢„æœŸæ•ˆæœ
- LaMAæ¨¡å‹å¯é€‰å®‰è£…
- ä¸å½±å“å…¶ä»–æ¨¡å‹åŠŸèƒ½
- æä¾›æ¸…æ™°çš„å®‰è£…æŒ‡å¯¼

### 1.3 é¢œè‰²ç©ºé—´å¤„ç†ä¿®å¤ ğŸŸ¡ ä¸­ä¼˜å…ˆçº§

#### é—®é¢˜æè¿°
- LaMAä½¿ç”¨BGRè¾“å…¥ï¼Œå…¶ä»–æ¨¡å‹ä½¿ç”¨RGB
- ç»Ÿä¸€å¤„ç†å¯¼è‡´é¢œè‰²å¼‚å¸¸
- éœ€è¦æ¨¡å‹ç‰¹å®šçš„é¢œè‰²è½¬æ¢

#### ä¿®å¤æ–¹æ¡ˆ

**1.3.1 å®ç°æ¨¡å‹ç‰¹å®šé¢„å¤„ç†**
```python
# åœ¨ core/models/base_model.py ä¸­æ·»åŠ 
class ColorSpaceProcessor:
    @staticmethod
    def prepare_image_for_model(image: np.ndarray, model_name: str) -> np.ndarray:
        """æ ¹æ®æ¨¡å‹ç±»å‹å‡†å¤‡å›¾åƒ"""
        if model_name == 'lama':
            # LaMAéœ€è¦BGRè¾“å…¥
            return cv2.cvtColor(image, cv2.COLOR_RGB2BGR)
        else:
            # å…¶ä»–æ¨¡å‹ä½¿ç”¨RGB
            return image
            
    @staticmethod
    def process_output_for_display(result: np.ndarray, model_name: str) -> np.ndarray:
        """å¤„ç†æ¨¡å‹è¾“å‡º"""
        if model_name == 'lama':
            # LaMAè¾“å‡ºBGRï¼Œè½¬æ¢ä¸ºRGBæ˜¾ç¤º
            return cv2.cvtColor(result, cv2.COLOR_BGR2RGB)
        else:
            # å…¶ä»–æ¨¡å‹è¾“å‡ºRGB
            return result
```

**1.3.2 æ›´æ–°å„æ¨¡å‹å¤„ç†å™¨**
```python
# åœ¨æ¯ä¸ªæ¨¡å‹å¤„ç†å™¨ä¸­æ·»åŠ é¢œè‰²ç©ºé—´å¤„ç†
def predict(self, image, mask, config=None):
    # é¢„å¤„ç†
    image_array = np.array(image.convert("RGB"))
    image_array = ColorSpaceProcessor.prepare_image_for_model(image_array, 'model_name')
    
    # æ¨¡å‹æ¨ç†
    result = self._model_inference(image_array, mask, config)
    
    # åå¤„ç†
    result = ColorSpaceProcessor.process_output_for_display(result, 'model_name')
    return result
```

#### é¢„æœŸæ•ˆæœ
- æ‰€æœ‰æ¨¡å‹é¢œè‰²è¾“å‡ºæ­£ç¡®
- ä¿æŒå¤„ç†æ€§èƒ½
- ç»Ÿä¸€æ˜¾ç¤ºæ ¼å¼

## 2. æ€§èƒ½ä¼˜åŒ–

### 2.1 æ˜¾å­˜ç®¡ç†ç­–ç•¥

#### 2.1.1 ç¯å¢ƒå˜é‡ä¼˜åŒ–
```bash
# æ·»åŠ åˆ°å¯åŠ¨è„šæœ¬
export PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True
export CUDA_LAUNCH_BLOCKING=1
```

#### 2.1.2 æ¨¡å‹åŠ è½½ç­–ç•¥
- é»˜è®¤åªåŠ è½½ZITSæ¨¡å‹
- å…¶ä»–æ¨¡å‹æŒ‰éœ€åŠ è½½
- å®ç°æ¨¡å‹ç¼“å­˜æœºåˆ¶

### 2.2 å¤„ç†æ€§èƒ½ä¼˜åŒ–

#### 2.2.1 å›¾åƒé¢„å¤„ç†ä¼˜åŒ–
```python
# ä¼˜åŒ–å›¾åƒé¢„å¤„ç†æµç¨‹
def optimize_image_processing(image, target_size):
    # æ™ºèƒ½ç¼©æ”¾ç­–ç•¥
    if max(image.size) > target_size:
        scale = target_size / max(image.size)
        new_size = tuple(int(dim * scale) for dim in image.size)
        image = image.resize(new_size, Image.LANCZOS)
    return image
```

#### 2.2.2 æ‰¹å¤„ç†æ”¯æŒ
```python
# æ”¯æŒæ‰¹é‡å¤„ç†
def batch_process(images, masks, config):
    # å®ç°æ‰¹é‡å¤„ç†é€»è¾‘
    pass
```

## 3. UI/UXæ”¹è¿›

### 3.1 Streamlitå‚æ•°æ›´æ–°
```python
# æ›¿æ¢å·²å¼ƒç”¨çš„å‚æ•°
st.image(image, use_container_width=True)  # æ›¿æ¢ use_column_width
```

### 3.2 æ¨¡å‹çŠ¶æ€æ˜¾ç¤º
```python
# æ·»åŠ æ¨¡å‹çŠ¶æ€é¢æ¿
def render_model_status():
    st.sidebar.subheader("ğŸ”§ Model Status")
    for model_name in ['zits', 'mat', 'fcf', 'lama']:
        status = "âœ…" if is_model_loaded(model_name) else "âŒ"
        st.sidebar.write(f"{status} {model_name.upper()}")
```

### 3.3 é”™è¯¯å¤„ç†æ”¹è¿›
```python
# æ›´å¥½çš„é”™è¯¯æç¤º
def handle_processing_error(error):
    if "CUDA out of memory" in str(error):
        st.error("æ˜¾å­˜ä¸è¶³ï¼Œè¯·å°è¯•ï¼š\n1. é™ä½å›¾åƒåˆ†è¾¨ç‡\n2. ä½¿ç”¨æ›´å°çš„æ¨¡å‹\n3. é‡å¯åº”ç”¨")
    elif "lama processor not loaded" in str(error):
        st.warning("LaMAæ¨¡å‹æœªåŠ è½½ï¼Œå°†ä½¿ç”¨å…¶ä»–å¯ç”¨æ¨¡å‹")
```

## 4. æµ‹è¯•éªŒè¯

### 4.1 è‡ªåŠ¨åŒ–æµ‹è¯•
```python
# tests/test_memory_management.py
def test_model_switching():
    """æµ‹è¯•æ¨¡å‹åˆ‡æ¢æ—¶çš„æ˜¾å­˜ç®¡ç†"""
    pass

def test_color_processing():
    """æµ‹è¯•é¢œè‰²ç©ºé—´å¤„ç†"""
    pass

def test_lama_fallback():
    """æµ‹è¯•LaMAé™çº§æœºåˆ¶"""
    pass
```

### 4.2 æ€§èƒ½åŸºå‡†æµ‹è¯•
```python
# benchmarks/performance_test.py
def benchmark_memory_usage():
    """æ˜¾å­˜ä½¿ç”¨åŸºå‡†æµ‹è¯•"""
    pass

def benchmark_processing_speed():
    """å¤„ç†é€Ÿåº¦åŸºå‡†æµ‹è¯•"""
    pass
```

## 5. éƒ¨ç½²ä¼˜åŒ–

### 5.1 ç¯å¢ƒé…ç½®
```yaml
# docker-compose.yml ä¼˜åŒ–
services:
  watermark-remover:
    environment:
      - PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True
      - CUDA_VISIBLE_DEVICES=0
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
```

### 5.2 ç›‘æ§é›†æˆ
```python
# æ·»åŠ Prometheusç›‘æ§
from prometheus_client import Counter, Histogram

processing_time = Histogram('processing_time_seconds', 'Time spent processing images')
memory_usage = Histogram('gpu_memory_usage_gb', 'GPU memory usage in GB')
```

## 6. å®æ–½è®¡åˆ’

### é˜¶æ®µ1: æ ¸å¿ƒä¿®å¤ (1-2å¤©)
- [x] ä¿®å¤ä¸»ç¨‹åºconfig_managerä¼ é€’é—®é¢˜
- [x] ä¿®å¤UI session_stateåˆå§‹åŒ–é—®é¢˜
- [ ] å®ç°æ˜¾å­˜ç®¡ç†ä¼˜åŒ–
- [ ] ä¿®å¤LaMAæ¨¡å—åŒ–æ”¯æŒ

### é˜¶æ®µ2: æ€§èƒ½ä¼˜åŒ– (2-3å¤©)
- [ ] å®ç°é¢œè‰²ç©ºé—´å¤„ç†
- [ ] ä¼˜åŒ–æ¨¡å‹åŠ è½½ç­–ç•¥
- [ ] æ·»åŠ æ˜¾å­˜ç›‘æ§

### é˜¶æ®µ3: UIæ”¹è¿› (1å¤©)
- [ ] æ›´æ–°Streamlitå‚æ•°
- [ ] æ·»åŠ æ¨¡å‹çŠ¶æ€æ˜¾ç¤º
- [ ] æ”¹è¿›é”™è¯¯å¤„ç†

### é˜¶æ®µ4: æµ‹è¯•éªŒè¯ (1å¤©)
- [ ] è‡ªåŠ¨åŒ–æµ‹è¯•
- [ ] æ€§èƒ½åŸºå‡†æµ‹è¯•
- [ ] ç”¨æˆ·éªŒæ”¶æµ‹è¯•

## 7. é£é™©è¯„ä¼°

### é«˜é£é™©
- æ˜¾å­˜ç®¡ç†ä¿®æ”¹å¯èƒ½å½±å“ç¨³å®šæ€§
- éœ€è¦å……åˆ†æµ‹è¯•

### ä¸­é£é™©
- LaMAæ¨¡å—åŒ–å¯èƒ½å¼•å…¥æ–°ä¾èµ–
- é¢œè‰²å¤„ç†ä¿®æ”¹éœ€è¦éªŒè¯

### ä½é£é™©
- UIæ”¹è¿›ä¸»è¦æ˜¯ç”¨æˆ·ä½“éªŒæå‡

## 8. æˆåŠŸæŒ‡æ ‡

### æŠ€æœ¯æŒ‡æ ‡
- æ˜¾å­˜ä½¿ç”¨å‡å°‘50%
- æ¨¡å‹åˆ‡æ¢æ—¶é—´ < 5ç§’
- æ‰€æœ‰æ¨¡å‹é¢œè‰²è¾“å‡ºæ­£ç¡®
- LaMAå¯é€‰å®‰è£…æˆåŠŸ

### ç”¨æˆ·ä½“éªŒæŒ‡æ ‡
- å¤„ç†æˆåŠŸç‡ > 95%
- é”™è¯¯æç¤ºæ¸…æ™°æ˜ç¡®
- ç•Œé¢å“åº”æµç•…

## 9. ç»´æŠ¤è®¡åˆ’

### å®šæœŸæ£€æŸ¥
- æ¯å‘¨æ£€æŸ¥æ˜¾å­˜ä½¿ç”¨æƒ…å†µ
- æ¯æœˆæ›´æ–°ä¾èµ–ç‰ˆæœ¬
- æ¯å­£åº¦æ€§èƒ½åŸºå‡†æµ‹è¯•

### ç›‘æ§å‘Šè­¦
- æ˜¾å­˜ä½¿ç”¨ç‡ > 80% å‘Šè­¦
- å¤„ç†å¤±è´¥ç‡ > 5% å‘Šè­¦
- å“åº”æ—¶é—´ > 30ç§’ å‘Šè­¦

---

## æ›´æ–°è®°å½•

| æ—¥æœŸ | ç‰ˆæœ¬ | ä¿®æ”¹å†…å®¹ | çŠ¶æ€ |
|------|------|----------|------|
| 2025-07-05 | v2.0 | æ·»åŠ CUDAå†…å­˜ç®¡ç†ã€LaMAæ¨¡å—åŒ–ã€é¢œè‰²ç©ºé—´ä¿®å¤ | ğŸ”„ è¿›è¡Œä¸­ |
| 2025-07-05 | v1.0 | åˆå§‹ä¿®å¤æ–¹æ¡ˆ | âœ… å®Œæˆ | 