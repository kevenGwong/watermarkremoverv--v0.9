#!/usr/bin/env python3
"""
æµ‹è¯•LaMAæ¨¡å‹æ˜¯å¦çœŸçš„ä½¿ç”¨promptå‚æ•°
"""
import numpy as np
from iopaint.model_manager import ModelManager
from iopaint.schema import InpaintRequest, HDStrategy, LDMSampler
import torch
import cv2
from PIL import Image

def test_lama_prompt():
    """æµ‹è¯•LaMAæ¨¡å‹æ˜¯å¦ä½¿ç”¨promptå‚æ•°"""
    print("ğŸ” æµ‹è¯•LaMAæ¨¡å‹æ˜¯å¦ä½¿ç”¨promptå‚æ•°...")
    
    # è®¾ç½®è®¾å¤‡
    device = "cuda" if torch.cuda.is_available() else "cpu"
    print(f"ä½¿ç”¨è®¾å¤‡: {device}")
    
    # åŠ è½½LaMAæ¨¡å‹
    print("æ­£åœ¨åŠ è½½LaMAæ¨¡å‹...")
    model_manager = ModelManager(name="lama", device=device)
    print("LaMAæ¨¡å‹åŠ è½½å®Œæˆ")
    
    # åˆ›å»ºä¸€ä¸ªç®€å•çš„æµ‹è¯•å›¾åƒå’Œmask
    test_image = np.random.randint(0, 255, (256, 256, 3), dtype=np.uint8)
    test_mask = np.zeros((256, 256), dtype=np.uint8)
    test_mask[100:150, 100:150] = 255  # åˆ›å»ºä¸€ä¸ªçŸ©å½¢mask
    
    print(f"æµ‹è¯•å›¾åƒå°ºå¯¸: {test_image.shape}")
    print(f"æµ‹è¯•maskå°ºå¯¸: {test_mask.shape}")
    
    # æµ‹è¯•1: ä¸ä½¿ç”¨prompt
    print("\nğŸ“ æµ‹è¯•1: ä¸ä½¿ç”¨prompt")
    config1 = InpaintRequest(
        ldm_steps=20,
        ldm_sampler=LDMSampler.ddim,
        hd_strategy=HDStrategy.CROP,
        hd_strategy_crop_margin=64,
        hd_strategy_crop_trigger_size=800,
        hd_strategy_resize_limit=1600,
        prompt="",
        negative_prompt=""
    )
    print(f"Config1 prompt: '{config1.prompt}'")
    print(f"Config1 negative_prompt: '{config1.negative_prompt}'")
    
    result1 = model_manager(test_image, test_mask, config1)
    print(f"ç»“æœ1å½¢çŠ¶: {result1.shape}")
    
    # æµ‹è¯•2: ä½¿ç”¨prompt
    print("\nğŸ“ æµ‹è¯•2: ä½¿ç”¨prompt")
    config2 = InpaintRequest(
        ldm_steps=20,
        ldm_sampler=LDMSampler.ddim,
        hd_strategy=HDStrategy.CROP,
        hd_strategy_crop_margin=64,
        hd_strategy_crop_trigger_size=800,
        hd_strategy_resize_limit=1600,
        prompt="natural background, seamless, realistic",
        negative_prompt="watermark, text, logo, artificial"
    )
    print(f"Config2 prompt: '{config2.prompt}'")
    print(f"Config2 negative_prompt: '{config2.negative_prompt}'")
    
    result2 = model_manager(test_image, test_mask, config2)
    print(f"ç»“æœ2å½¢çŠ¶: {result2.shape}")
    
    # æ¯”è¾ƒç»“æœ
    print("\nğŸ” ç»“æœæ¯”è¾ƒ:")
    diff = np.abs(result1.astype(float) - result2.astype(float))
    max_diff = np.max(diff)
    mean_diff = np.mean(diff)
    print(f"æœ€å¤§å·®å¼‚: {max_diff:.2f}")
    print(f"å¹³å‡å·®å¼‚: {mean_diff:.2f}")
    
    if max_diff < 1.0:
        print("âœ… ç»“è®º: LaMAæ¨¡å‹ä¸ä½¿ç”¨promptå‚æ•° - ä¸¤ä¸ªç»“æœå‡ ä¹å®Œå…¨ç›¸åŒ")
    else:
        print("â“ ç»“è®º: LaMAæ¨¡å‹å¯èƒ½ä½¿ç”¨promptå‚æ•° - ç»“æœæœ‰å·®å¼‚")
    
    # æ£€æŸ¥LaMAæ¨¡å‹çš„forwardæ–¹æ³•æ˜¯å¦ä½¿ç”¨configå‚æ•°
    print("\nğŸ” æ£€æŸ¥LaMAæ¨¡å‹æºç :")
    from iopaint.model.lama import LaMa
    import inspect
    
    lama_model = model_manager.model
    print(f"LaMAæ¨¡å‹ç±»å‹: {type(lama_model)}")
    
    # æ£€æŸ¥forwardæ–¹æ³•æ˜¯å¦ä½¿ç”¨configå‚æ•°
    source = inspect.getsource(lama_model.forward)
    lines = source.split('\n')
    
    print("Forwardæ–¹æ³•ä¸­ä½¿ç”¨configçš„è¡Œ:")
    config_usage = []
    for i, line in enumerate(lines):
        if 'config.' in line:
            config_usage.append(f"ç¬¬{i+1}è¡Œ: {line.strip()}")
    
    if config_usage:
        for usage in config_usage:
            print(f"  {usage}")
    else:
        print("  âŒ æ²¡æœ‰æ‰¾åˆ°configå‚æ•°çš„ä½¿ç”¨")
    
    print("\nğŸ“‹ æ€»ç»“:")
    print("1. InpaintRequestç¡®å®æœ‰promptå’Œnegative_promptå­—æ®µ")
    print("2. ä½†LaMAæ¨¡å‹çš„forwardæ–¹æ³•æ²¡æœ‰ä½¿ç”¨è¿™äº›å‚æ•°")
    print("3. åªæœ‰SDç­‰æ‰©æ•£æ¨¡å‹æ‰çœŸæ­£ä½¿ç”¨promptå‚æ•°")
    print("4. åœ¨LaMAæ¨¡å‹ä¸­ä¼ é€’promptå‚æ•°æ˜¯æ— æ•ˆçš„")

if __name__ == "__main__":
    test_lama_prompt() 