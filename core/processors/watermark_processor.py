"""
æ°´å°å¤„ç†ä¸»ç±» - æ¨¡å—åŒ–ç‰ˆæœ¬
"""

import logging
import time
import yaml
import numpy as np
from typing import Dict, Any, Optional
from PIL import Image
from pathlib import Path

from .processing_result import ProcessingResult
from ..models.mask_generators import CustomMaskGenerator, FlorenceMaskGenerator, FallbackMaskGenerator
from ..models.lama_processor import LamaProcessor
from ..models.iopaint_processor import IOPaintProcessor

logger = logging.getLogger(__name__)

class WatermarkProcessor:
    """æ°´å°å¤„ç†ä¸»ç±» - åŸºäºåŸå§‹ web_backend.py"""
    
    def __init__(self, config_path: Optional[str] = None):
        if config_path is None:
            config_path = Path(__file__).parent.parent.parent / "web_config.yaml"
        self.config_path = config_path
        self.config = self._load_config(config_path)
        self._resources = []  # Track resources for cleanup
        
        # Initialize components
        self._init_components()
    
    def __enter__(self):
        """Context manager entry"""
        return self
    
    def __exit__(self, exc_type, exc_val, exc_tb):
        """Context manager exit with automatic cleanup"""
        self.cleanup_resources()
    
    def cleanup_resources(self):
        """Clean up all processor resources"""
        try:
            # Clean up LaMA processor
            if hasattr(self, 'lama_processor') and self.lama_processor:
                if hasattr(self.lama_processor, 'cleanup_resources'):
                    self.lama_processor.cleanup_resources()
            
            # Clean up mask generator if it has cleanup method
            if hasattr(self, 'mask_generator') and hasattr(self.mask_generator, 'cleanup_resources'):
                self.mask_generator.cleanup_resources()
            
            # Clear CUDA cache
            import torch
            if torch.cuda.is_available():
                torch.cuda.empty_cache()
                
            logger.info("âœ… WatermarkProcessor resources cleaned up")
        except Exception as e:
            logger.warning(f"âš ï¸ Error during WatermarkProcessor cleanup: {e}")
    
    def _init_components(self):
        """Initialize processor components"""
        # åˆå§‹åŒ–maskç”Ÿæˆå™¨
        mask_type = self.config['mask_generator']['model_type']
        try:
            if mask_type == "custom":
                self.mask_generator = CustomMaskGenerator(self.config)
            else:
                self.mask_generator = FlorenceMaskGenerator(self.config)
        except Exception as e:
            logger.error(f"Failed to initialize mask generator: {e}")
            # æä¾›é™çº§æ–¹æ¡ˆ - ä½¿ç”¨åŸºç¡€çš„ç©ºmaskç”Ÿæˆå™¨
            self.mask_generator = FallbackMaskGenerator()
            logger.info("Using fallback mask generator")
        
        # åˆå§‹åŒ–LaMAå¤„ç†å™¨
        try:
            self.lama_processor = LamaProcessor(self.config)
        except Exception as e:
            logger.error(f"Failed to initialize LaMA processor: {e}")
            raise RuntimeError(f"Critical failure: LaMA processor initialization failed: {e}")
        
        logger.info(f"âœ… WatermarkProcessor initialized with {mask_type} mask generator")
    
    def _load_config(self, config_path: str) -> Dict[str, Any]:
        """åŠ è½½é…ç½®æ–‡ä»¶"""
        try:
            with open(config_path, 'r', encoding='utf-8') as f:
                config = yaml.safe_load(f)
                logger.info(f"âœ… Config loaded from: {config_path}")
                return config
        except Exception as e:
            logger.warning(f"Config file {config_path} not found, using defaults")
            # ä½¿ç”¨ConfigManagerçš„é»˜è®¤é…ç½®
            from config.config import ConfigManager
            config_manager = ConfigManager()
            return self._build_default_config(config_manager)
    
    def _build_default_config(self, config_manager) -> Dict[str, Any]:
        """æ„å»ºé»˜è®¤é…ç½®"""
        return {
            'mask_generator': {
                'model_type': 'custom',
                'mask_model_path': '/home/duolaameng/SAM_Remove/Watermark_sam/output/checkpoints/epoch=071-valid_iou=0.7267.ckpt',
                'image_size': 768,
                'imagenet_mean': [0.485, 0.456, 0.406],
                'imagenet_std': [0.229, 0.224, 0.225],
                'mask_threshold': config_manager.app_config.default_mask_threshold,
            },
            'models': {
                'florence_model': 'microsoft/Florence-2-large',
                'lama_model': config_manager.get_model_config().get('lama_model', 'lama')
            }
        }
    
    def process_image(self, 
                     image: Image.Image,
                     transparent: bool = False,
                     max_bbox_percent: float = 10.0,
                     force_format: Optional[str] = None,
                     custom_inpaint_config: Optional[Dict[str, Any]] = None) -> ProcessingResult:
        """å¤„ç†å•å¼ å›¾ç‰‡"""
        start_time = time.time()
        
        try:
            logger.info("ğŸš€ å¼€å§‹å¤„ç†å›¾åƒ...")
            logger.info(f"ğŸ“¸ è¾“å…¥å›¾åƒ: size={image.size}, mode={image.mode}")
            logger.info(f"ğŸ¯ å¤„ç†æ¨¡å¼: {'é€æ˜' if transparent else 'ä¿®å¤'}")
            
            # ç”Ÿæˆmask
            logger.info("ğŸ­ å¼€å§‹ç”Ÿæˆmask...")
            mask_params = {'max_bbox_percent': max_bbox_percent}
            mask_image = self.mask_generator.generate_mask(image, mask_params)
            
            # éªŒè¯mask
            mask_array = np.array(mask_image)
            white_pixels = np.sum(mask_array > 128)
            total_pixels = mask_array.size
            mask_coverage = white_pixels / total_pixels * 100
            logger.info(f"ğŸ” MaskéªŒè¯: è¦†ç›–ç‡={mask_coverage:.2f}%")
            
            if transparent:
                logger.info("ğŸ«¥ åº”ç”¨é€æ˜å¤„ç†...")
                result_image = self._make_region_transparent(image, mask_image)
            else:
                logger.info("ğŸ¨ åº”ç”¨LaMAä¿®å¤å¤„ç†...")
                if custom_inpaint_config is None:
                    custom_inpaint_config = {}
                result_image_array = self.lama_processor.predict(image, mask_image, custom_inpaint_config)
                result_image = Image.fromarray(result_image_array)
            
            processing_time = time.time() - start_time
            logger.info(f"âœ… å¤„ç†å®Œæˆï¼Œè€—æ—¶: {processing_time:.2f}ç§’")
            
            return ProcessingResult(
                success=True,
                result_image=result_image,
                mask_image=mask_image,
                processing_time=processing_time
            )
            
        except Exception as e:
            processing_time = time.time() - start_time
            logger.error(f"âŒ å›¾åƒå¤„ç†å¤±è´¥: {e}")
            return ProcessingResult(
                success=False,
                error_message=str(e),
                processing_time=processing_time
            )
    
    def _make_region_transparent(self, image: Image.Image, mask: Image.Image) -> Image.Image:
        """ä½¿åŒºåŸŸé€æ˜"""
        image = image.convert("RGBA")
        mask = mask.convert("L")
        transparent_image = Image.new("RGBA", image.size)
        
        for x in range(image.width):
            for y in range(image.height):
                if mask.getpixel((x, y)) > 0:
                    transparent_image.putpixel((x, y), (0, 0, 0, 0))
                else:
                    transparent_image.putpixel((x, y), image.getpixel((x, y)))
        
        return transparent_image
    
    def _process_with_lama(self, image: Image.Image, mask: Image.Image, lama_config: Dict[str, Any]) -> Image.Image:
        """ä½¿ç”¨LaMAè¿›è¡Œinpainting - å…¼å®¹æ¥å£"""
        result_array = self.lama_processor.predict(image, mask, lama_config)
        return Image.fromarray(result_array)
    
    def get_system_info(self) -> Dict[str, Any]:
        """è·å–ç³»ç»Ÿä¿¡æ¯"""
        import torch
        import psutil
        
        info = {
            "cuda_available": torch.cuda.is_available(),
            "lama_loaded": self.lama_processor.model_loaded,
            "mask_generator": self.config['mask_generator']['model_type'],
            "ram_usage": f"{psutil.virtual_memory().percent:.1f}%",
            "cpu_usage": f"{psutil.cpu_percent():.1f}%"
        }
        
        if torch.cuda.is_available():
            gpu_info = torch.cuda.get_device_properties(0)
            vram_total = gpu_info.total_memory // (1024 ** 2)
            vram_used = torch.cuda.memory_reserved(0) // (1024 ** 2)
            info["vram_usage"] = f"{vram_used}/{vram_total} MB"
            info["gpu_name"] = gpu_info.name
        
        return info

class EnhancedWatermarkProcessor:
    """å¢å¼ºçš„æ°´å°å¤„ç†å™¨ - æ”¯æŒ IOPaint"""
    
    def __init__(self, base_processor: WatermarkProcessor):
        self.base_processor = base_processor
        self.iopaint_processor = None
        self._load_iopaint_processor()
    
    def _load_iopaint_processor(self):
        """Load IOPaint processor"""
        try:
            # Create IOPaint config
            config = {
                'models': {
                    'inpaint_model': 'mat',  # é»˜è®¤ä½¿ç”¨MAT
                    'available_models': ['zits', 'mat', 'fcf', 'lama']
                },
                'iopaint_config': {
                    'hd_strategy': 'CROP',
                    'hd_strategy_crop_margin': 64,
                    'hd_strategy_crop_trigger_size': 1024,
                    'hd_strategy_resize_limit': 2048,
                    'ldm_steps': 50,
                    'auto_model_selection': True
                }
            }
            
            self.iopaint_processor = IOPaintProcessor(config)
            logger.info("âœ… IOPaint processor loaded successfully")
            
        except Exception as e:
            logger.warning(f"IOPaint processor loading failed: {e}")
            logger.info("IOPaint functionality will not be available")
    
    def process_image_with_params(self, 
                                image: Image.Image,
                                mask_model: str,
                                mask_params: Dict[str, Any],
                                inpaint_params: Dict[str, Any],
                                performance_params: Dict[str, Any],
                                transparent: bool = False) -> ProcessingResult:
        """ä½¿ç”¨è¯¦ç»†å‚æ•°å¤„ç†å›¾åƒ"""
        start_time = time.time()
        
        try:
            logger.info("ğŸš€ å¼€å§‹å¢å¼ºå¤„ç†æµç¨‹...")
            logger.info(f"ğŸ­ Maskæ¨¡å‹: {mask_model}")
            logger.info(f"âš™ï¸ Inpaintå‚æ•°: {inpaint_params}")
            
            # ç”Ÿæˆmask
            if mask_model == "upload":
                mask_image = self._generate_uploaded_mask(image, mask_params)
            elif mask_model == "florence2":
                mask_image = self.base_processor.mask_generator.generate_mask(image, mask_params)
            else:  # custom
                mask_image = self.base_processor.mask_generator.generate_mask(image, mask_params)
            
            # éªŒè¯mask
            mask_array = np.array(mask_image)
            white_pixels = np.sum(mask_array > 128)
            total_pixels = mask_array.size
            mask_coverage = white_pixels / total_pixels * 100
            logger.info(f"ğŸ” MaskéªŒè¯: è¦†ç›–ç‡={mask_coverage:.2f}%")
            
            # åº”ç”¨å¤„ç†
            if transparent:
                logger.info("ğŸ«¥ åº”ç”¨é€æ˜å¤„ç†...")
                result_image = self.base_processor._make_region_transparent(image, mask_image)
            else:
                inpaint_model = inpaint_params.get('inpaint_model', 'lama')
                
                if inpaint_model == 'iopaint' and self.iopaint_processor and self.iopaint_processor.model_loaded:
                    logger.info("ğŸ¨ åº”ç”¨IOPaintå¤„ç†...")
                    result_array = self.iopaint_processor.predict(image, mask_image, inpaint_params)
                    result_image = Image.fromarray(result_array)
                else:
                    logger.info("ğŸ¨ åº”ç”¨LaMAå¤„ç†...")
                    result_array = self.base_processor.lama_processor.predict(image, mask_image, inpaint_params)
                    result_image = Image.fromarray(result_array)
            
            processing_time = time.time() - start_time
            logger.info(f"âœ… å¤„ç†å®Œæˆï¼Œè€—æ—¶: {processing_time:.2f}ç§’")
            
            return ProcessingResult(
                success=True,
                result_image=result_image,
                mask_image=mask_image,
                processing_time=processing_time
            )
            
        except Exception as e:
            processing_time = time.time() - start_time
            logger.error(f"âŒ å¢å¼ºå¤„ç†å¤±è´¥: {e}")
            return ProcessingResult(
                success=False,
                error_message=str(e),
                processing_time=processing_time
            )
    
    def _generate_uploaded_mask(self, image: Image.Image, params: Dict[str, Any]) -> Image.Image:
        """å¤„ç†ä¸Šä¼ çš„mask"""
        uploaded_mask = params.get('uploaded_mask')
        if not uploaded_mask:
            raise ValueError("No mask file uploaded")
        
        # Validate uploaded mask
        if hasattr(uploaded_mask, 'size') and uploaded_mask.size == 0:
            raise ValueError("Uploaded mask file is empty")
        
        try:
            logger.info(f"ğŸ“‚ å¤„ç†ä¸Šä¼ çš„maskæ–‡ä»¶")
            logger.info(f"ğŸ“ åŸå›¾å°ºå¯¸: {image.size}")
            
            # è¯»å–ä¸Šä¼ çš„mask
            if hasattr(uploaded_mask, 'read'):
                # Streamlit UploadedFileå¯¹è±¡
                uploaded_mask.seek(0)  # é‡ç½®æ–‡ä»¶æŒ‡é’ˆ
                mask = Image.open(uploaded_mask)
            elif isinstance(uploaded_mask, Image.Image):
                # å·²ç»æ˜¯PIL Imageå¯¹è±¡
                mask = uploaded_mask
            else:
                # æ–‡ä»¶è·¯å¾„
                mask = Image.open(uploaded_mask)
                
        except Exception as e:
            raise ValueError(f"Failed to open uploaded mask file: {e}")
        
        # Validate mask after loading
        if mask.size[0] <= 0 or mask.size[1] <= 0:
            raise ValueError("Invalid mask dimensions")
        
        logger.info(f"ğŸ“ åŸå§‹maskå°ºå¯¸: {mask.size}")
        logger.info(f"ğŸ¨ åŸå§‹maskæ¨¡å¼: {mask.mode}")
        
        # ç¡®ä¿maskæ˜¯ç°åº¦å›¾åƒ
        if mask.mode != 'L':
            mask = mask.convert('L')
            logger.info(f"ğŸ”„ è½¬æ¢maskä¸ºç°åº¦æ¨¡å¼: {mask.mode}")
        
        # è°ƒæ•´maskå°ºå¯¸ä»¥åŒ¹é…å›¾åƒ
        if mask.size != image.size:
            logger.info(f"ğŸ“ è°ƒæ•´maskå°ºå¯¸: {mask.size} -> {image.size}")
            mask = mask.resize(image.size, Image.LANCZOS)
        else:
            logger.info(f"âœ… Maskå°ºå¯¸å·²åŒ¹é…: {mask.size}")
        
        # æ£€æŸ¥maskå†…å®¹
        mask_array = np.array(mask)
        white_pixels = np.sum(mask_array > 128)
        total_pixels = mask_array.size
        mask_coverage = white_pixels / total_pixels * 100
        logger.info(f"ğŸ” Maskå†…å®¹åˆ†æ: ç™½è‰²åƒç´ ={white_pixels}, æ€»åƒç´ ={total_pixels}, è¦†ç›–ç‡={mask_coverage:.2f}%")
        logger.info(f"ğŸ“Š Maskåƒç´ å€¼èŒƒå›´: {mask_array.min()} - {mask_array.max()}")
        
        # åº”ç”¨é¢å¤–çš„è†¨èƒ€å¤„ç†ï¼ˆå¦‚æœéœ€è¦ï¼‰
        dilate_size = params.get('mask_dilate_kernel_size', 0)
        if dilate_size > 0:
            import cv2
            logger.info(f"ğŸ”§ åº”ç”¨è†¨èƒ€å¤„ç†: kernel_size={dilate_size}")
            mask_array = np.array(mask)
            kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (dilate_size, dilate_size))
            mask_array = cv2.dilate(mask_array, kernel, iterations=1)
            mask = Image.fromarray(mask_array, mode='L')
            
            # æ£€æŸ¥è†¨èƒ€åçš„mask
            white_pixels_after = np.sum(mask_array > 128)
            coverage_after = white_pixels_after / total_pixels * 100
            logger.info(f"ğŸ” è†¨èƒ€åMaskåˆ†æ: ç™½è‰²åƒç´ ={white_pixels_after}, è¦†ç›–ç‡={coverage_after:.2f}%")
        
        logger.info(f"âœ… æœ€ç»ˆmaskå°ºå¯¸: {mask.size}, æ¨¡å¼: {mask.mode}")
        return mask 