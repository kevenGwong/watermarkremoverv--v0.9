"""
çœŸæ­£çš„ç»Ÿä¸€å¤„ç†å™¨
æ ¹æ®ç”¨æˆ·é€‰æ‹©åŠ¨æ€åˆ‡æ¢æ¨¡å‹ï¼Œæ”¯æŒZITSã€MATã€FCFã€LaMA
"""

import logging
from typing import Dict, Any, Optional
from PIL import Image
import numpy as np

from .base_inpainter import BaseInpainter, ModelRegistry

logger = logging.getLogger(__name__)

class UnifiedProcessor:
    """
    ç»Ÿä¸€å¤„ç†å™¨ - æ ¹æ®ç”¨æˆ·é€‰æ‹©åŠ¨æ€åˆ‡æ¢æ¨¡å‹
    æ›¿ä»£ä¹‹å‰é”™è¯¯çš„ UnifiedProcessor = SimplifiedLamaProcessor å®ç°
    """
    
    def __init__(self, config: Dict[str, Any]):
        self.config = config
        self.current_processor = None
        self.current_model_name = None
        self.loaded_processors = {}  # ç¼“å­˜å·²åŠ è½½çš„å¤„ç†å™¨
        
        # æ³¨å†Œæ‰€æœ‰å¯ç”¨æ¨¡å‹
        self._register_models()
        
        logger.info("âœ… UnifiedProcessor initialized with dynamic model switching")
    
    def _register_models(self):
        """æ³¨å†Œæ‰€æœ‰å¯ç”¨çš„æ¨¡å‹ç±»"""
        try:
            from .zits_processor import ZitsProcessor
            from .mat_processor import MatProcessor
            from .fcf_processor import FcfProcessor
            from .lama_processor_simplified import SimplifiedLamaProcessor
            
            # æ³¨å†Œæ‰€æœ‰æ¨¡å‹
            ModelRegistry.register('zits', ZitsProcessor)
            ModelRegistry.register('mat', MatProcessor)
            ModelRegistry.register('fcf', FcfProcessor)
            ModelRegistry.register('lama', SimplifiedLamaProcessor)
            
            logger.info("âœ… All models registered successfully")
            
        except ImportError as e:
            logger.error(f"âŒ Failed to register models: {e}")
            raise
    
    def switch_model(self, model_name: str) -> bool:
        """åˆ‡æ¢åˆ°æŒ‡å®šæ¨¡å‹"""
        try:
            if model_name == self.current_model_name and self.current_processor is not None:
                logger.info(f"âœ… Model {model_name} already loaded and active")
                return True
            
            # æ£€æŸ¥æ¨¡å‹æ˜¯å¦å¯ç”¨
            available_models = self.get_available_models()
            if model_name not in available_models:
                logger.error(f"âŒ Model {model_name} not available. Available: {available_models}")
                return False
            
            # æ¸…ç†å½“å‰å¤„ç†å™¨ï¼ˆä½†ä¿ç•™åœ¨ç¼“å­˜ä¸­ä»¥å¤‡å¤ç”¨ï¼‰
            if self.current_processor is not None:
                logger.info(f"ğŸ”„ Switching from {self.current_model_name} to {model_name}")
            
            # æ£€æŸ¥æ˜¯å¦å·²ç»åŠ è½½è¿‡è¯¥æ¨¡å‹
            if model_name in self.loaded_processors:
                logger.info(f"â™»ï¸ Reusing cached {model_name} processor")
                self.current_processor = self.loaded_processors[model_name]
            else:
                # åˆ›å»ºæ–°çš„å¤„ç†å™¨å®ä¾‹
                logger.info(f"ğŸš€ Loading new {model_name} processor...")
                processor = ModelRegistry.create_model(model_name, self.config)
                processor._load_model()
                
                # ç¼“å­˜å¤„ç†å™¨
                self.loaded_processors[model_name] = processor
                self.current_processor = processor
                
                logger.info(f"âœ… {model_name.upper()} processor loaded and cached")
            
            self.current_model_name = model_name
            return True
            
        except Exception as e:
            logger.error(f"âŒ Failed to switch to model {model_name}: {e}")
            return False
    
    def predict_with_model(self, image: Image.Image, mask: Image.Image, config: Dict[str, Any]) -> np.ndarray:
        """ä½¿ç”¨é€‰å®šçš„æ¨¡å‹è¿›è¡Œé¢„æµ‹"""
        # ä»é…ç½®ä¸­è·å–æ¨¡å‹åç§°
        model_name = config.get('model_name')
        if not model_name:
            raise ValueError("No model specified in config")
        
        # åˆ‡æ¢åˆ°æŒ‡å®šæ¨¡å‹
        if not self.switch_model(model_name):
            raise RuntimeError(f"Failed to switch to model: {model_name}")
        
        # ç¡®ä¿æœ‰å½“å‰å¤„ç†å™¨
        if self.current_processor is None:
            raise RuntimeError("No processor available after model switch")
        
        logger.info(f"ğŸ¨ Processing with {model_name.upper()} model")
        
        # æ‰§è¡Œé¢„æµ‹
        try:
            result = self.current_processor.predict(image, mask, config)
            logger.info(f"âœ… {model_name.upper()} processing completed successfully")
            return result
            
        except Exception as e:
            logger.error(f"âŒ {model_name.upper()} processing failed: {e}")
            raise
    
    def get_available_models(self) -> list:
        """è·å–æ‰€æœ‰å¯ç”¨æ¨¡å‹åˆ—è¡¨"""
        return ModelRegistry.get_available_models()
    
    def get_current_model(self) -> str:
        """è·å–å½“å‰æ´»åŠ¨çš„æ¨¡å‹åç§°"""
        return self.current_model_name or "none"
    
    def is_model_loaded(self, model_name: str) -> bool:
        """æ£€æŸ¥æŒ‡å®šæ¨¡å‹æ˜¯å¦å·²åŠ è½½"""
        return model_name in self.loaded_processors
    
    def get_loaded_models(self) -> list:
        """è·å–å·²åŠ è½½çš„æ¨¡å‹åˆ—è¡¨"""
        return list(self.loaded_processors.keys())
    
    def cleanup_resources(self):
        """æ¸…ç†æ‰€æœ‰å¤„ç†å™¨èµ„æº"""
        try:
            logger.info("ğŸ§¹ Cleaning up UnifiedProcessor resources...")
            
            # æ¸…ç†æ‰€æœ‰å·²åŠ è½½çš„å¤„ç†å™¨
            for model_name, processor in self.loaded_processors.items():
                try:
                    processor.cleanup_resources()
                    logger.info(f"âœ… {model_name.upper()} processor cleaned up")
                except Exception as e:
                    logger.warning(f"âš ï¸ Error cleaning up {model_name}: {e}")
            
            # æ¸…ç©ºç¼“å­˜
            self.loaded_processors.clear()
            self.current_processor = None
            self.current_model_name = None
            
            logger.info("âœ… UnifiedProcessor cleanup completed")
            
        except Exception as e:
            logger.error(f"âŒ Error during UnifiedProcessor cleanup: {e}")
    
    def get_system_info(self) -> Dict[str, Any]:
        """è·å–ç³»ç»Ÿä¿¡æ¯"""
        return {
            "current_model": self.current_model_name,
            "available_models": self.get_available_models(),
            "loaded_models": self.get_loaded_models(),
            "total_cached_processors": len(self.loaded_processors)
        }
    
    def __enter__(self):
        """ä¸Šä¸‹æ–‡ç®¡ç†å™¨å…¥å£"""
        return self
    
    def __exit__(self, exc_type, exc_val, exc_tb):
        """ä¸Šä¸‹æ–‡ç®¡ç†å™¨é€€å‡º - è‡ªåŠ¨æ¸…ç†èµ„æº"""
        self.cleanup_resources()